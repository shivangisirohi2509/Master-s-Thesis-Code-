{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Outlet X final.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Calendar Year Week'] = df['Calendar Year Week'].astype(str)\n",
    "df['Fiscal Year Week'] = df['Fiscal Year Week'].astype(str)\n",
    "df[['Fiscal Week', 'Fiscal Year']] = df['Fiscal Year Week'].str.split('.', expand=True)\n",
    "df['Fiscal Week'] = df['Fiscal Week'].astype(int)\n",
    "df['Fiscal Year'] = df['Fiscal Year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with zeros in the numerical columns\n",
    "numerical_columns = [\n",
    "    'Delta In Transit Qty', \n",
    "    'Sales Qty',  \n",
    "]\n",
    "\n",
    "for column in numerical_columns:\n",
    "    df[column] = df[column].fillna(0)\n",
    "\n",
    "# Check if there are any NaN values left in the dataframe\n",
    "nan_values = df.isna().sum()\n",
    "\n",
    "nan_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Delta In Transit Qty', 'Sales Qty', and 'Sales Qty (incl returns)' columns to integers\n",
    "df['Delta In Transit Qty'] = df['Delta In Transit Qty'].astype(int)\n",
    "df['Delta In Transit Qty'] = df['Delta In Transit Qty'].apply(lambda x: 0 if x < 0 else x)\n",
    "df['Sales Qty'] = df['Sales Qty'].astype(int)\n",
    "# Convert Calendar Year Week and Calendar Day into datetime format\n",
    "df['Calendar Day'] = pd.to_datetime(df['Calendar Day'])\n",
    "#make a month column from calendar day column \n",
    "df['Month'] = df['Calendar Day'].dt.month\n",
    "df['Month Name'] = df['Calendar Day'].dt.strftime('%B')\n",
    "\n",
    "# Make Sales Qty and Sales Qty (incl returns) positive\n",
    "df['Sales Qty'] = df['Sales Qty'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if opening and colosing inventory is <0 \n",
    "negative_opening_inventory = df[df['Opening Balance Qty'] < 0].shape[0]\n",
    "negative_closing_inventory = df[df['Closing Balance Qty'] < 0].shape[0]\n",
    "\n",
    "negative_opening_inventory, negative_closing_inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check when the opening balance qty is negative but sales qty is positive\n",
    "negative_opening_positive_sales = df[(df['Opening Balance Qty'] == 0) & (df['Sales Qty'] > 0)].shape[0] \n",
    "negative_opening_positive_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inv_yn column if the opening balance qty is greater than 0 then 1 else 0\n",
    "df['Inv_yn'] = df['Opening Balance Qty'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code redundant can be deleted\n",
    "df['Inv_yn'] = df['Opening Balance Qty'].apply(lambda x: 1 if x > 0 else 0) #Condition: Opening inventory =0 but sales quantity is not equal to 0\n",
    "mask = df['Inv_yn'] == 0\n",
    "\n",
    "# Check if any 'Sales Qty' values are non-zero when 'Inv_yn' is 0\n",
    "condition_check = (df.loc[mask, 'Sales Qty'] > 0).any()\n",
    "\n",
    "# If condition_check is False, it means there are some rows where 'Inv_yn' is 0 but 'Sales Qty' is not 0\n",
    "# If condition_check is True, it means that whenever 'Inv_yn' is 0, 'Sales Qty' is also 0\n",
    "print(not condition_check)\n",
    "\n",
    "# Create a DataFrame where 'Inv_yn' is 0 but 'Sales Qty' is not 0\n",
    "df_inconsistent = df[mask & (df['Sales Qty'] > 0)]\n",
    "\n",
    "# Count the number of inconsistent rows\n",
    "num_inconsistent = len(df_inconsistent)\n",
    "\n",
    "print(num_inconsistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask where 'Inv_yn' is 0 and 'Closing Balance Qty' is greater than 0 and 'Delta In Transit Qty' is greater than 0\n",
    "mask = (df['Inv_yn'] == 0) & (df['Closing Balance Qty'] > 0) & (df['Delta In Transit Qty'] > 0) & (df['Sales Qty'] > 0)\n",
    "# Update the 'Opening Balance Qty' column to 'Closing Balance Qty' + 'Sales Qty' where the mask is True\n",
    "df.loc[mask, 'Opening Balance Qty'] = df['Closing Balance Qty'] + df['Sales Qty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inv_yn column if the opening balance qty is greater than 0 then 1 else 0\n",
    "df['Inv_yn'] = df['Opening Balance Qty'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['Inv_yn'] == 0\n",
    "\n",
    "# Check if any 'Sales Qty' values are non-zero when 'Inv_yn' is 0\n",
    "condition_check = (df.loc[mask, 'Sales Qty'] != 0).any()\n",
    "\n",
    "# If condition_check is False, it means there are some rows where 'Inv_yn' is 0 but 'Sales Qty' is not 0\n",
    "# If condition_check is True, it means that whenever 'Inv_yn' is 0, 'Sales Qty' is also 0\n",
    "print(not condition_check)\n",
    "\n",
    "# Create a DataFrame where 'Inv_yn' is 0 but 'Sales Qty' is not 0\n",
    "df_inconsistent = df[mask & (df['Sales Qty'] != 0)]\n",
    "\n",
    "# Count the number of inconsistent rows\n",
    "num_inconsistent = len(df_inconsistent)\n",
    "\n",
    "print(num_inconsistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Stockout' that is True when both 'Inv_yn' and 'Sales Qty' are 0\n",
    "df['Stockout'] = (df['Inv_yn'] == 0) & (df['Sales Qty'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022 = df[df['Fiscal Year'] == 2022]\n",
    "df_2023 = df[df['Fiscal Year'] == 2023]\n",
    "\n",
    "#calculate the percentage of stockouts in the filtered dataframe for 2022 and 2023\n",
    "stockout_percentage_2022 = (df_2022['Stockout'].sum() / len(df_2022)) * 100\n",
    "stockout_percentage_2023 = (df_2023['Stockout'].sum() / len(df_2023)) * 100\n",
    "\n",
    "print(stockout_percentage_2022, stockout_percentage_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reindex(columns=['Calendar Day','Weekday', 'Fiscal Year', 'Fiscal Week', 'Fiscal Year Week', 'Calendar Year Week', 'Month', 'Month Name', 'FMS Site', 'FMS Site Name', 'Gender','Article: MH5 Product Group', 'Article: MH6 Sub Product Group', 'Article: Generic ', 'Article: Generic Name', 'Article: Color Description', 'Article', 'Article Name', 'Article: Size ', 'Opening Balance Qty', 'Delta In Transit Qty', 'Sales Qty', 'Closing Balance Qty', 'Inv_yn', 'Stockout'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the stockouts correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table to count 'Stockout' days\n",
    "stockout_days_2022 = df_2022.pivot_table(index=['Article', 'Article Name'], \n",
    "                                         columns='Stockout', \n",
    "                                         aggfunc='size', \n",
    "                                         fill_value=0)\n",
    "\n",
    "stockout_days_2023 = df_2023.pivot_table(index=['Article', 'Article Name'], \n",
    "                                         columns='Stockout', \n",
    "                                         aggfunc='size', \n",
    "                                         fill_value=0)\n",
    "\n",
    "# Calculate the total number of days each article was stockout in 2022 and 2023\n",
    "stockout_days_2022['Total Days Stockout'] = stockout_days_2022[True]\n",
    "stockout_days_2023['Total Days Stockout'] = stockout_days_2023[True]\n",
    "\n",
    "# Drop the False column as it's not needed\n",
    "stockout_days_2022.drop(columns=False, inplace=True)\n",
    "stockout_days_2023.drop(columns=False, inplace=True)\n",
    "\n",
    "# Reset the index so 'Article' and 'Article Name' become columns again\n",
    "stockout_days_2022.reset_index(inplace=True)\n",
    "stockout_days_2023.reset_index(inplace=True)\n",
    "\n",
    "#drop the true column from the stockout_days_2022 and stockout_days_2023\n",
    "stockout_days_2022.drop(columns=True, inplace=True)\n",
    "stockout_days_2023.drop(columns=True, inplace=True)\n",
    "\n",
    "stockout_days_2022.head(), stockout_days_2023.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the article with 371 total days of stockout in 2023 \n",
    "article_371_days_2023 = stockout_days_2023[stockout_days_2023['Total Days Stockout'] == 371]\n",
    "\n",
    "# Display the article number\n",
    "print(article_371_days_2023['Article'].values)\n",
    "\n",
    "# Count the number of articles with 371 days of stockout in 2023\n",
    "num_articles_371_days_2023 = len(article_371_days_2023)\n",
    "\n",
    "print(num_articles_371_days_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the article with 364 total days of stockout in 2022\n",
    "article_364_days_2022 = stockout_days_2022[stockout_days_2022['Total Days Stockout'] == 364]\n",
    "\n",
    "# Display the article number\n",
    "print(article_364_days_2022['Article'].values)\n",
    "\n",
    "# Count the number of articles with 364 days of stockout in 2022\n",
    "num_articles_364_days_2022 = len(article_364_days_2022)\n",
    "\n",
    "print(num_articles_364_days_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropped Articles with complete stockout in 2022 and 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows with articles with 364 days of stockout in 2022 from the stockout_days_2022 and from dataframe df for only fiscal year 2022 since it contains data with both year 2022 and 2023.\n",
    "mask = df['Article'].isin(article_364_days_2022['Article']) & (df['Fiscal Year'] == '2022')\n",
    "df.drop(df[mask].index, inplace=True)\n",
    "\n",
    "# Drop the rows with articles with 364 days of stockout in 2022 from the stockout_days_2022 DataFrame\n",
    "mask = stockout_days_2022['Article'].isin(article_364_days_2022['Article'])\n",
    "stockout_days_2022.drop(stockout_days_2022[mask].index, inplace=True)\n",
    "stockout_days_2022.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# drop the rows with articles with 371 days of stockout in 2023 from the stockout_days_2023 and from dataframe df for only fiscal year 2023 since it contains data with both year 2022 and 2023.\n",
    "mask = df['Article'].isin(article_371_days_2023['Article']) & (df['Fiscal Year'] == '2023')\n",
    "df.drop(df[mask].index, inplace=True)   \n",
    "\n",
    "# Drop the rows with articles with 371 days of stockout in 2023 from the stockout_days_2023 DataFrame\n",
    "mask = stockout_days_2023['Article'].isin(article_371_days_2023['Article'])\n",
    "stockout_days_2023.drop(stockout_days_2023[mask].index, inplace=True)\n",
    "\n",
    "# Reset the index of the stockout_days_2023 DataFrame\n",
    "stockout_days_2023.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique articles in df for fiscal year 2022 and 2023\n",
    "unique_articles_2022 = df[df['Fiscal Year'] == 2022]['Article'].nunique()\n",
    "unique_articles_2023 = df[df['Fiscal Year'] == 2023]['Article'].nunique()\n",
    "\n",
    "unique_articles_2022, unique_articles_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of the total days stockout in 2022 and 2023\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(stockout_days_2022['Total Days Stockout'], bins=20, color='skyblue')\n",
    "plt.title('Total Days Stockout in 2022')\n",
    "plt.xlabel('Number of Days')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(stockout_days_2023['Total Days Stockout'], bins=20, color='salmon')\n",
    "plt.title('Total Days Stockout in 2023')\n",
    "plt.xlabel('Number of Days')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase in fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Calendar Day'] = pd.to_datetime(df['Calendar Day'])\n",
    "# Sort the DataFrame by 'Article' and 'Calendar Day'\n",
    "df = df.sort_values(by=['Article', 'Calendar Day'])\n",
    "first_nonzero_inventory_day = df[df['Opening Balance Qty'] > 0].groupby('Article')['Calendar Day'].min()\n",
    "\n",
    "# Define the start and end of February 2022\n",
    "february_2022_start = pd.Timestamp('2022-02-01')\n",
    "february_2022_end = pd.Timestamp('2022-02-28')\n",
    "\n",
    "# Adjust the first_nonzero_inventory_day to consider any day in February as potentially already in stock\n",
    "first_nonzero_inventory_day_adjusted = first_nonzero_inventory_day.apply(\n",
    "    lambda x: pd.Timestamp('2022-01-31') if february_2022_start <= x <= february_2022_end else x\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check for stockouts\n",
    "def check_stockout(row):\n",
    "    if row['Article'] in first_nonzero_inventory_day_adjusted:\n",
    "        # Only flag as stockout if the date is after the first adjusted arrival and both 'Inv_yn' and 'Sales Qty' are 0\n",
    "        return row['Calendar Day'] >= first_nonzero_inventory_day_adjusted[row['Article']] and row['Inv_yn'] == 0 and row['Sales Qty'] == 0\n",
    "    else:\n",
    "        # If the article has never arrived (according to adjusted dates), it cannot be a stockout\n",
    "        return False\n",
    "    \n",
    " # Apply the function across the DataFrame\n",
    "df['Stockout1'] = df.apply(check_stockout, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the rows where the opening balance qty is 0 while the stockout1 is false\n",
    "mask = (df['Opening Balance Qty'] == 0) & (df['Stockout1'] == False)\n",
    "\n",
    "# View the rows that will be dropped\n",
    "rows_to_drop = df[mask]\n",
    "print(rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Article' and 'Article Name' columns and count the number of days each article was stockout\n",
    "stockout_days_article = df.groupby(['Article', 'Article Name', 'Stockout1']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate the total number of days each article was stockout\n",
    "stockout_days_article['Total Days Stockout'] = stockout_days_article[True]\n",
    "\n",
    "# Drop the 'False' and 'True' columns\n",
    "stockout_days_article.drop(columns=[False, True], inplace=True)\n",
    "\n",
    "# Sort the DataFrame by 'Total Days Stockout' in descending order\n",
    "stockout_days_article = stockout_days_article.sort_values(by='Total Days Stockout', ascending=False)\n",
    "\n",
    "stockout_days_article.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase out fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing these articles and not updating the stockout since we dont want to calculate the lost sales of the articles that are phased out or not replenished\n",
    "\n",
    "df = df.sort_values(['Article', 'Calendar Day'])\n",
    "# Group by 'Article' and calculate the cumulative sum of consecutive stockout days\n",
    "df['Cumulative_Stockout'] = df.groupby('Article')['Stockout1'].cumsum() - df.groupby('Article')['Stockout1'].cumsum().where(~df['Stockout1']).ffill().fillna(0)\n",
    "\n",
    "# Find the maximum number of consecutive stockout days for each article\n",
    "max_consecutive_stockout = df.groupby('Article')['Cumulative_Stockout'].max().reset_index()\n",
    "\n",
    "# Filter articles with a maximum of at least 90 consecutive stockout days\n",
    "articles_with_long_stockouts = max_consecutive_stockout[max_consecutive_stockout['Cumulative_Stockout'] >= 30]\n",
    "\n",
    "# This gives you a DataFrame with articles that have had at least one 30-day consecutive stockout period\n",
    "articles_with_long_stockouts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative stockouts and reset the counter after a replenishment\n",
    "df['Cumulative_Stockout'] = (df.groupby(['Article', 'Article Name'])['Stockout1']\n",
    "                              .cumsum() - df.groupby(['Article', 'Article Name'])['Stockout1']\n",
    "                              .cumsum().where(~df['Stockout1']).ffill().fillna(0))\n",
    "\n",
    "# Identify all the unique articles that have ever had a stockout\n",
    "articles_stocked_out = df[df['Stockout1']]['Article'].unique()\n",
    "\n",
    "# Initialize an empty list to hold articles that meet the criteria\n",
    "articles_no_replenishment_after_stockout = []\n",
    "\n",
    "# Loop through each article that had a stockout\n",
    "for article in articles_stocked_out:\n",
    "    # Filter the DataFrame for the current article\n",
    "    article_df = df[df['Article'] == article]\n",
    "    # Check if there's any period of 90 consecutive days of stockout\n",
    "    if any(article_df['Cumulative_Stockout'] >= 30):\n",
    "        # Find the last stockout day\n",
    "        last_stockout_day = article_df[article_df['Cumulative_Stockout'] >= 30]['Calendar Day'].max()\n",
    "        # Check if there's any replenishment after the last stockout day\n",
    "        replenishment_after_stockout = article_df[(article_df['Calendar Day'] > last_stockout_day) & \n",
    "                                                  (article_df['Opening Balance Qty'] > 0)]\n",
    "        # If there's no replenishment, add the article to the list\n",
    "        if replenishment_after_stockout.empty:\n",
    "            articles_no_replenishment_after_stockout.append(article)\n",
    "\n",
    "# Create a DataFrame with the articles and their names that had no replenishment after a 90-day stockout\n",
    "final_df = df[df['Article'].isin(articles_no_replenishment_after_stockout)][['Article', 'Article Name']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase out dropped articles check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the article numbers in final_df from the original dataframe df\n",
    "df = df[~df['Article'].isin(final_df['Article'])]\n",
    "\n",
    "#reset the index of the final_df\n",
    "final_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_articles_2022 = df[df['Fiscal Year'] == 2022]['Article'].nunique()\n",
    "unique_articles_2023 = df[df['Fiscal Year'] == 2023]['Article'].nunique()\n",
    "\n",
    "unique_articles_2022, unique_articles_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the percentage of stockouts in the filtered dataframe\n",
    "stockout_percentage = (df['Stockout'].sum() / len(df)) * 100\n",
    "\n",
    "print(stockout_percentage)\n",
    "\n",
    "#seperately for year fiscal 2022 and 2023 for the filtered dataframe\n",
    "df_2022 = df[df['Fiscal Year'] == 2022]\n",
    "df_2023 = df[df['Fiscal Year'] == 2023]\n",
    "\n",
    "#calculate the percentage of stockouts in the filtered dataframe for 2022 and 2023\n",
    "stockout_percentage_2022 = (df_2022['Stockout'].sum() / len(df_2022)) * 100\n",
    "stockout_percentage_2023 = (df_2023['Stockout'].sum() / len(df_2023)) * 100\n",
    "\n",
    "print(stockout_percentage_2022, stockout_percentage_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stockout days per aticle after the phase in fix\n",
    "# Group by 'Article' and 'Article Name' columns and count the number of days each article was stockout\n",
    "stockout_days_article = df.groupby(['Article', 'Article Name', 'Stockout1']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate the total number of days each article was stockout\n",
    "stockout_days_article['Total Days Stockout'] = stockout_days_article[True]\n",
    "\n",
    "# Drop the 'False' and 'True' columns\n",
    "stockout_days_article.drop(columns=[False, True], inplace=True)\n",
    "\n",
    "# Sort the DataFrame by 'Total Days Stockout' in descending order\n",
    "stockout_days_article = stockout_days_article.sort_values(by='Total Days Stockout', ascending=False)\n",
    "stockout_days_article.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Article' and 'Fiscal Year Week' and calculate the number of days each article was stockout in a week \n",
    "stockout_days_week = df.groupby(['Article', 'Fiscal Year Week'])['Stockout1'].sum().reset_index()\n",
    "stockout_days_week.head()\n",
    "# Merge the original DataFrame with the stockout_days_week DataFrame\n",
    "df = pd.merge(df, stockout_days_week, how='left', on=['Article', 'Fiscal Year Week'])\n",
    "\n",
    "# The new column 'Stockout1_y' in df now represents the total stockout days in a week for each day\n",
    "df.rename(columns={'Stockout1_y': 'Stockout Days in Week', 'Stockout1_x': 'Stockout1'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations stockout, sales, inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stockout pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can i calculate the weekly stockout percent for fiscal year 2022 and 2023 in subplots nexts to each other by fiscal week\n",
    "\n",
    "# Filter data for 2022 and 2023\n",
    "df_2022 = df[df['Fiscal Year'] == 2022]\n",
    "df_2023 = df[df['Fiscal Year'] == 2023]\n",
    "\n",
    "# Calculate weekly stockout percent\n",
    "stockout_2022_weekly = df_2022.groupby('Fiscal Week')['Stockout1'].mean() * 100\n",
    "stockout_2023_weekly = df_2023.groupby('Fiscal Week')['Stockout1'].mean() * 100\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot data for 2022\n",
    "axs[0].bar(stockout_2022_weekly.index, stockout_2022_weekly.values)\n",
    "axs[0].set_title('2022 Weekly Stockout Percent')\n",
    "axs[0].set_xlabel('Fiscal Week')\n",
    "axs[0].set_ylabel('Stockout Percent')\n",
    "axs[0].set_xticks(range(1, 52,2))\n",
    "\n",
    "# Plot data for 2023\n",
    "axs[1].bar(stockout_2023_weekly.index, stockout_2023_weekly.values)\n",
    "axs[1].set_title('2023 Weekly Stockout Percent')\n",
    "axs[1].set_xlabel('Fiscal Week')\n",
    "axs[1].set_ylabel('Stockout Percent')\n",
    "axs[1].set_xticks(range(1, 53,2))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for 2022 and 2023\n",
    "df_2022 = df[df['Fiscal Year'] == 2022]\n",
    "df_2023 = df[df['Fiscal Year'] == 2023]\n",
    "\n",
    "# Calculate weekly sales sum\n",
    "sales_2022_weekly = df_2022.groupby('Fiscal Week')['Sales Qty'].sum()\n",
    "sales_2023_weekly = df_2023.groupby('Fiscal Week')['Sales Qty'].sum()\n",
    "\n",
    "sales_2022_weekly = sales_2022_weekly / sales_2022_weekly.sum()\n",
    "sales_2023_weekly = sales_2023_weekly / sales_2023_weekly.sum()\n",
    "\n",
    "# Plot the sales proportion for 2022 and 2023 in subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot sales proportions for 2022\n",
    "axs[0].bar(sales_2022_weekly.index, label='2022')\n",
    "axs[0].set_title('Sales Proportions for 2022')\n",
    "axs[0].set_xlabel('Fiscal Week')\n",
    "axs[0].set_ylabel('Sales Proportions')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot sales proportions for 2023\n",
    "axs[1].bar(sales_2023_weekly.index, label='2023')\n",
    "axs[1].set_title('Sales Proportions for 2023')\n",
    "axs[1].set_xlabel('Fiscal Week')\n",
    "axs[1].set_ylabel('Sales Proportions')\n",
    "axs[1].legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for 2022 and 2023\n",
    "df_2022 = df[df['Fiscal Year'] == 2022]\n",
    "df_2023 = df[df['Fiscal Year'] == 2023]\n",
    "\n",
    "# Calculate weekly sales sum\n",
    "sales_2022_weekly = df_2022.groupby('Fiscal Week')['Sales Qty'].sum()\n",
    "sales_2023_weekly = df_2023.groupby('Fiscal Week')['Sales Qty'].sum()\n",
    "\n",
    "sales_2022_weekly = sales_2022_weekly / sales_2022_weekly.sum()\n",
    "sales_2023_weekly = sales_2023_weekly / sales_2023_weekly.sum()\n",
    "\n",
    "# Plot the sales proportion for 2022 and 2023 in subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot sales proportions for 2022\n",
    "axs[0].bar(sales_2022_weekly.index, sales_2022_weekly, label='2022')\n",
    "axs[0].set_title('Sales Proportions for 2022')\n",
    "axs[0].set_xlabel('Fiscal Week')\n",
    "axs[0].set_ylabel('Sales Proportions')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot sales proportions for 2023\n",
    "axs[1].bar(sales_2023_weekly.index, sales_2023_weekly, label='2023')\n",
    "axs[1].set_title('Sales Proportions for 2023')\n",
    "axs[1].set_xlabel('Fiscal Week')\n",
    "axs[1].set_ylabel('Sales Proportions')\n",
    "axs[1].legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two Series into a DataFrame\n",
    "stockout_weekly = pd.concat([stockout_2022_weekly, stockout_2023_weekly], axis=1)\n",
    "stockout_weekly.columns = ['2022', '2023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average stockout percent for each group\n",
    "weekly_stockout_sub = df.groupby(['Fiscal Week', 'Fiscal Year', 'Article: MH6 Sub Product Group'])['Stockout1'].mean() * 100\n",
    "# Reset index to make 'Fiscal Week' and 'Article: MH6 Sub Product Group' into columns\n",
    "weekly_stockout_sub = weekly_stockout_sub.reset_index()\n",
    "\n",
    "# Calculate min, max, and mean of weekly stockout percent for each group\n",
    "stockout_min_sub = weekly_stockout_sub.groupby('Article: MH6 Sub Product Group')['Stockout1'].min()\n",
    "stockout_max_sub = weekly_stockout_sub.groupby('Article: MH6 Sub Product Group')['Stockout1'].max()\n",
    "stockout_mean_sub = weekly_stockout_sub.groupby('Article: MH6 Sub Product Group')['Stockout1'].mean()\n",
    "\n",
    "# Combine min, max, and mean of weekly stockout percent into one DataFrame\n",
    "stockout_summary_sub = pd.DataFrame({'Min Stockout%': stockout_min_sub, 'Max Stockout%': stockout_max_sub, 'Mean Stockout%': stockout_mean_sub})\n",
    "\n",
    "# Display the first 12 rows of the DataFrame\n",
    "stockout_summary_sub.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average weekly sales for each group\n",
    "weekly_sales_sub = df.groupby(['Fiscal Week', 'Fiscal Year', 'Article: MH6 Sub Product Group'])['Sales Qty'].sum()\n",
    "\n",
    "# Reset index to make 'Fiscal Week' and 'Article: MH6 Sub Product Group' into columns\n",
    "weekly_sales_sub = weekly_sales_sub.reset_index()\n",
    "\n",
    "# Calculate min, max, and mean of weekly sales for each group\n",
    "sales_min_sub = weekly_sales_sub.groupby('Article: MH6 Sub Product Group')['Sales Qty'].min()\n",
    "sales_max_sub = weekly_sales_sub.groupby('Article: MH6 Sub Product Group')['Sales Qty'].max()\n",
    "sales_mean_sub = weekly_sales_sub.groupby('Article: MH6 Sub Product Group')['Sales Qty'].mean()\n",
    "\n",
    "# Combine min, max, and mean of weekly sales into one DataFrame\n",
    "sales_summary_sub = pd.DataFrame({'Min Sales': sales_min_sub, 'Max Sales': sales_max_sub, 'Mean Sales': sales_mean_sub})\n",
    "\n",
    "# Display the first 12 rows of the DataFrame\n",
    "sales_summary_sub.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inventory Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_inventory_df = df.groupby(['Article', 'Fiscal Year', 'Fiscal Week']).agg({\n",
    "    'Article: MH5 Product Group': 'first',\n",
    "    'Article: MH6 Sub Product Group': 'first',\n",
    "    'Opening Balance Qty': 'first',\n",
    "    'Closing Balance Qty': 'last',\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate min, max, and mean of weekly inventory for each group\n",
    "inventory_min_sub = weekly_inventory_df.groupby('Article: MH6 Sub Product Group')['Opening Balance Qty'].min()\n",
    "inventory_max_sub = weekly_inventory_df.groupby('Article: MH6 Sub Product Group')['Opening Balance Qty'].max()\n",
    "inventory_mean_sub = weekly_inventory_df.groupby('Article: MH6 Sub Product Group')['Opening Balance Qty'].mean()\n",
    "\n",
    "# Combine min, max, and mean of weekly inventory into one DataFrame\n",
    "inventory_summary_sub = pd.DataFrame({'Min Inventory': inventory_min_sub, 'Max Inventory': inventory_max_sub, 'Mean Inventory': inventory_mean_sub})\n",
    "\n",
    "inventory_summary_sub = inventory_summary_sub.round(0)\n",
    "# Display the first 12 rows of the DataFrame\n",
    "inventory_summary_sub.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_summary_sub.to_excel(\"Inventory Summary Sub Product Group.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the df to excel\n",
    "df.to_excel(\"Outlet X cleaned.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Censored Demand Estimation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intra Week Cyclicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your dataframe is named df\n",
    "# Set the correct order for weekdays\n",
    "weekday_order = ['MO', 'TU', 'WE', 'TH', 'FR', 'SA', 'SU']\n",
    "df['Weekday'] = pd.Categorical(df['Weekday'], categories=weekday_order, ordered=True)\n",
    "\n",
    "# Calculate overall sales weight by weekday\n",
    "overall_sales_weight = df.groupby('Weekday')['Sales Qty'].sum() / df['Sales Qty'].sum() * 100\n",
    "\n",
    "# Calculate sales weight by product group\n",
    "product_group_sales_weight = df.groupby(['Weekday', 'Article: MH5 Product Group'])['Sales Qty'].sum().unstack().apply(lambda x: x / x.sum() * 100)\n",
    "\n",
    "# Calculate sales weight by sub product group\n",
    "sub_product_group_sales_weight = df.groupby(['Weekday', 'Article: MH6 Sub Product Group'])['Sales Qty'].sum().unstack().apply(lambda x: x / x.sum() * 100)\n",
    "\n",
    "# Plot the data\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 12))\n",
    "\n",
    "# Overall Sales Weight by Weekday\n",
    "axes[0].bar(overall_sales_weight.index, overall_sales_weight.values)\n",
    "axes[0].set_title('Overall Sales Weight by Weekday', fontsize=10)\n",
    "axes[0].set_xlabel('Weekday', fontsize=8)\n",
    "axes[0].set_ylabel('Sales Weight (%)', fontsize=8)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "# Sales Weight by Weekday and Product Group\n",
    "product_group_sales_weight.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Sales Weight by Weekday and Product Group', fontsize=10)\n",
    "axes[1].set_xlabel('Weekday', fontsize=8)\n",
    "axes[1].set_ylabel('Sales Weight (%)', fontsize=8)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=8)\n",
    "axes[1].set_xticklabels(product_group_sales_weight.index, rotation=0) \n",
    "axes[1].legend(fontsize=6)\n",
    "\n",
    "# Sales Weight by Weekday and Sub Product Group\n",
    "sub_product_group_sales_weight.plot(kind='bar', ax=axes[2])\n",
    "axes[2].set_title('Sales Weight by Weekday and Sub Product Group', fontsize=10)\n",
    "axes[2].set_xlabel('Weekday', fontsize=8)\n",
    "axes[2].set_ylabel('Sales Weight (%)', fontsize=8)\n",
    "axes[2].tick_params(axis='both', which='major', labelsize=8)\n",
    "axes[2].set_xticklabels(product_group_sales_weight.index, rotation=0) \n",
    "axes[2].legend(fontsize=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sales_weight_by_weekday.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the sales weight for each weekday for Fiscal year 2022 and 2023 for all the articles \n",
    "# Create a DataFrame for each year\n",
    "df_2022 = df[df['Fiscal Year'] == 2022]\n",
    "df_2023 = df[df['Fiscal Year'] == 2023]\n",
    "\n",
    "# Group by 'Weekday' and calculate the total sales for each weekday\n",
    "total_sales_weekday_2022 = df_2022.groupby('Weekday')['Sales Qty'].sum()\n",
    "total_sales_weekday_2023 = df_2023.groupby('Weekday')['Sales Qty'].sum()\n",
    "\n",
    "# Calculate the total sales for each year\n",
    "total_sales_2022 = total_sales_weekday_2022.sum()\n",
    "total_sales_2023 = total_sales_weekday_2023.sum()\n",
    "\n",
    "# Calculate the sales weight for each weekday\n",
    "sales_weight_weekday_2022 = (total_sales_weekday_2022 / total_sales_2022*100).round(0)\n",
    "sales_weight_weekday_2023 = (total_sales_weekday_2023 / total_sales_2023*100).round(0)\n",
    "\n",
    "sales_weight_weekday_2022, sales_weight_weekday_2023\n",
    "\n",
    "#Sort the sales weight for each weekday by the day of the week Monday to Sunday \"MO\" to \"SU\" and round to 0 decimal places but the sum should be 100\n",
    "\n",
    "sales_weight_weekday_2022 = sales_weight_weekday_2022.reindex(['MO', 'TU', 'WE', 'TH', 'FR', 'SA', 'SU'])\n",
    "sales_weight_weekday_2023 = sales_weight_weekday_2023.reindex(['MO', 'TU', 'WE', 'TH', 'FR', 'SA', 'SU'])\n",
    "sales_weight_weekday_2022, sales_weight_weekday_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the sum of sales weight is 100 by rounding down the sales for one weekday\n",
    "sales_weight_weekday_2022['MO'] = 100 - sales_weight_weekday_2022['TU':'SU'].sum()\n",
    "\n",
    "sales_weight_weekday_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the sum of sales weight is 100 by rounding down the sales for one weekday \"TH\"\n",
    "sales_weight_weekday_2023.loc['TH'] = 100 - sales_weight_weekday_2023.loc[['MO', 'TU', 'WE', 'FR', 'SA', 'SU']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average sales weight for each weekday\n",
    "average_sales_weight_weekday = (sales_weight_weekday_2022 + sales_weight_weekday_2023) / 2\n",
    "\n",
    "# Round the sales weight to 0 decimal places\n",
    "average_sales_weight_weekday = average_sales_weight_weekday.round(0)\n",
    "\n",
    "# Ensure the sum of sales weight is 100 by rounding down the sales for one weekday\n",
    "average_sales_weight_weekday['MO'] = 100 - average_sales_weight_weekday['TU':'SU'].sum()\n",
    "\n",
    "average_sales_weight_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the average sales weight to the 'Weekday' column\n",
    "df['Sales Weight'] = df['Weekday'].map(average_sales_weight_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weighted Inventory'] = df['Inv_yn'] * df['Sales Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly = df.groupby(['Article', 'Fiscal Year', 'Fiscal Week']).agg({\n",
    "    'FMS Site': 'first',\n",
    "    'Calendar Day': 'first',\n",
    "    'Month': 'first',\n",
    "    'Month Name': 'first',\n",
    "    'Fiscal Year Week': 'first',\n",
    "    'Calendar Year Week': 'first',\n",
    "    'FMS Site Name': 'first',\n",
    "    'Gender': 'first',\n",
    "    'Article: MH5 Product Group': 'first',\n",
    "    'Article: MH6 Sub Product Group': 'first',\n",
    "    'Article: Generic ': 'first',\n",
    "    'Article: Generic Name': 'first',\n",
    "    'Article: Color Description': 'first',\n",
    "    'Article Name': 'first',\n",
    "    'Article: Size ': 'first',\n",
    "    'Sales Qty': 'sum',\n",
    "    'Weighted Inventory': 'sum',\n",
    "    'Opening Balance Qty': 'first',\n",
    "    'Delta In Transit Qty': 'sum',\n",
    "    'Closing Balance Qty': 'last',\n",
    "    'Stockout Days in Week': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "#I want to multiply the adjusted sales by 100 to get the percentage of adjusted sales and want to round off the values to 0 decimal places\n",
    "df_weekly['Adjusted Sales'] = (df_weekly['Sales Qty'] / df_weekly['Weighted Inventory']) * 100\n",
    "df_weekly['Adjusted Sales'] = df_weekly['Adjusted Sales'].round(0)\n",
    "df_weekly = df_weekly.sort_values(by=['Article','Fiscal Year', 'Fiscal Week'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lost sales intra-week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summation of adjusted sales and sales qty total\n",
    "adjusted_sales_total = df_weekly['Adjusted Sales'].sum()\n",
    "sales_qty_total = df_weekly['Sales Qty'].sum()\n",
    "\n",
    "adjusted_sales_total, sales_qty_total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter- Weekly Impuatation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Article: MH5 Product Group', 'Fiscal Year', and 'Fiscal Week' and calculate the total sales for each product group\n",
    "total_sales_product_group = df.groupby(['Article: MH5 Product Group', 'Fiscal Year', 'Fiscal Week'])['Sales Qty'].sum()\n",
    "total_sales_product_group = total_sales_product_group.reset_index()\n",
    "average_sales_product_group = total_sales_product_group.groupby(['Article: MH5 Product Group', 'Fiscal Year'])['Sales Qty'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'total_sales_product_group' and 'average_sales_product_group' on 'Article: MH5 Product Group' and 'Fiscal Year'\n",
    "merged_df = pd.merge(total_sales_product_group, average_sales_product_group, on=['Article: MH5 Product Group', 'Fiscal Year'], suffixes=('_total', '_average'))\n",
    "\n",
    "# Compute the weekly sales rate\n",
    "merged_df['weekly_sales_rate'] = merged_df['Sales Qty_total'] / merged_df['Sales Qty_average']\n",
    "\n",
    "# Round the 'weekly_sales_rate' to 1 decimal place\n",
    "merged_df['weekly_sales_rate'] = merged_df['weekly_sales_rate'].round(1)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Article: MH5 Product Group', 'Fiscal Year', and 'Fiscal Week' and calculate the total sales for each product group\n",
    "total_sales_subproduct_group = df.groupby(['Article: MH6 Sub Product Group', 'Fiscal Year', 'Fiscal Week'])['Sales Qty'].sum()\n",
    "\n",
    "# Reset the index to make the DataFrame easier to work with\n",
    "total_sales_subproduct_group = total_sales_subproduct_group.reset_index()\n",
    "\n",
    "# Group by 'Article: MH5 Product Group' and 'Fiscal Year' and calculate the average total sales for each product group\n",
    "average_sales_subproduct_group = total_sales_subproduct_group.groupby(['Article: MH6 Sub Product Group', 'Fiscal Year'])['Sales Qty'].mean()\n",
    "\n",
    "# Reset the index to make the DataFrame easier to work with\n",
    "average_sales_subproduct_group = average_sales_subproduct_group.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'total_sales_product_group' and 'average_sales_product_group' on 'Article: MH5 Product Group' and 'Fiscal Year'\n",
    "merged_df_sub = pd.merge(total_sales_subproduct_group, average_sales_subproduct_group, on=['Article: MH6 Sub Product Group', 'Fiscal Year'], suffixes=('_total', '_average'))\n",
    "\n",
    "# Compute the weekly sales rate\n",
    "merged_df_sub['weekly_sales_rate'] = merged_df_sub['Sales Qty_total'] / merged_df_sub['Sales Qty_average']\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "merged_df_sub.head()\n",
    "\n",
    "#round to one decimal place\n",
    "# Round the 'weekly_sales_rate' to 1 decimal place\n",
    "merged_df_sub['weekly_sales_rate'] = merged_df_sub['weekly_sales_rate'].round(1)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "merged_df_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panty Table article vs product group and sub product group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Product Group\n",
    "# Get unique years\n",
    "years = [2022, 2023]\n",
    "\n",
    "# Loop over each year\n",
    "for year in years:\n",
    "    # Create a new figure for each year\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Filter the data for the sub product group \"CK UDW Men Boxers Boxer Briefs\" and the specific year\n",
    "    product_group_data_year = merged_df[(merged_df['Article: MH5 Product Group'] == 'Women Panty Table') & (merged_df['Fiscal Year'] == year)]\n",
    "\n",
    "    # Filter the data for the specific article and the specific year\n",
    "    article_data_year = merged_article_df[merged_article_df['Fiscal Year'] == year]\n",
    "\n",
    "    # Plot the weekly sales rate for the sub product group\n",
    "    plt.plot(product_group_data_year['Fiscal Week'], product_group_data_year['weekly_sales_rate'], label=f'Women Panty Table {year}')\n",
    "    plt.plot(article_data_year['Fiscal Week'], article_data_year['weekly_sales_rate'], label=f'Article X {year}')\n",
    "    plt.legend()\n",
    "    plt.title(f'Weekly Sales Rate for  Women Panty Table and Article X for {year}')\n",
    "    plt.xlabel('Fiscal Week')\n",
    "    plt.ylabel('Weekly Sales Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique years\n",
    "years = [2022, 2023]\n",
    "\n",
    "# Loop over each year\n",
    "for year in years:\n",
    "    # Create a new figure for each year\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Filter the data for the sub product group \"CK UDW Men Boxers Boxer Briefs\" and the specific year\n",
    "    sub_product_group_data_year = merged_df_sub[(merged_df_sub['Article: MH6 Sub Product Group'] == 'Women Panty Table Thongs') & (merged_df_sub['Fiscal Year'] == year)]\n",
    "\n",
    "    # Filter the data for the specific article and the specific year\n",
    "    article_data_year = merged_article_df[merged_article_df['Fiscal Year'] == year]\n",
    "\n",
    "    # Plot the weekly sales rate for the sub product group\n",
    "    plt.plot(sub_product_group_data_year['Fiscal Week'], sub_product_group_data_year['weekly_sales_rate'], label=f'Women Panty Table Thongs {year}')\n",
    "\n",
    "    # Plot the weekly sales rate for the specific article\n",
    "    plt.plot(article_data_year['Fiscal Week'], article_data_year['weekly_sales_rate'], label=f'Article X {year}')\n",
    "    plt.legend()\n",
    "    plt.title(f'Weekly Sales Rate for Women Panty Table Thongs and Article X for {year}')\n",
    "    plt.xlabel('Fiscal Week')\n",
    "    plt.ylabel('Weekly Sales Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique years\n",
    "years = [2022, 2023]\n",
    "\n",
    "# Loop over each year\n",
    "for year in years:\n",
    "    # Filter the data for the product group \"CK UDW Women Panty Table\" and the specific year\n",
    "    product_group_data_year = merged_df[\n",
    "        (merged_df['Article: MH5 Product Group'] == 'CK UDW Women Panty Table') & \n",
    "        (merged_df['Fiscal Year'] == year)\n",
    "    ]\n",
    "\n",
    "    # Filter the data for the specific article and the specific year\n",
    "    article_data_year = merged_article_df[\n",
    "        merged_article_df['Fiscal Year'] == year\n",
    "    ]\n",
    "\n",
    "    # Check if there is data to analyze\n",
    "    if not product_group_data_year.empty and not article_data_year.empty:\n",
    "        # Sort data by 'Fiscal Week' to ensure alignment\n",
    "        product_group_data_year = product_group_data_year.sort_values('Fiscal Week')\n",
    "        article_data_year = article_data_year.sort_values('Fiscal Week')\n",
    "\n",
    "        # Merge the dataframes on 'Fiscal Week'\n",
    "        merged_year_data = pd.merge(\n",
    "            product_group_data_year, \n",
    "            article_data_year, \n",
    "            on='Fiscal Week', \n",
    "            suffixes=('_prod', '_art')\n",
    "        )\n",
    "\n",
    "        # Calculate the correlation between the weekly sales rates of the two datasets\n",
    "        if not merged_year_data.empty:\n",
    "            correlation = merged_year_data['weekly_sales_rate_prod'].corr(merged_year_data['weekly_sales_rate_art'])\n",
    "            print(f\"The correlation between weekly sales rates for the product group and the article for {year} is: {correlation:.3f}\")\n",
    "        else:\n",
    "            print(f\"No overlapping data to calculate correlation for the year {year}.\")\n",
    "    else:\n",
    "        print(f\"No data available for either product group or article for {year}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only the rows where 'Stockout Days in Week' is 0\n",
    "df_no_stockouts = df_weekly[df_weekly['Stockout Days in Week'] == 0]\n",
    "df_no_stockouts.rename(columns={'Article Name_x': 'Article Name'}, inplace=True)\n",
    "\n",
    "# Group by 'Article', 'Article Name', 'Fiscal Year', and 'Fiscal Week', then calculate the mean of 'Sales Qty'\n",
    "average_weekly_sales = df_no_stockouts.groupby(['Article', 'Article Name', 'Fiscal Year'])['Sales Qty'].mean()\n",
    "\n",
    "# Reset the index to make the DataFrame easier to work with\n",
    "average_weekly_sales = average_weekly_sales.reset_index()\n",
    "\n",
    "# Rename the 'Sales Qty' column to 'Average Weekly Sales'\n",
    "average_weekly_sales.rename(columns={'Sales Qty': 'Average Weekly Sales'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'df_weekly' and 'average_weekly_sales' on 'Article' and 'Fiscal Year'\n",
    "df_weekly = pd.merge(df_weekly, average_weekly_sales, how='left', on=['Article', 'Article Name', 'Fiscal Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'df_weekly' and 'merged_df' on 'Article: MH5 Product Group', 'Fiscal Year', and 'Fiscal Week'\n",
    "df_weekly = pd.merge(df_weekly, merged_df[['Article: MH5 Product Group', 'Fiscal Year', 'Fiscal Week', 'weekly_sales_rate']], how='left', on=['Article: MH5 Product Group', 'Fiscal Year', 'Fiscal Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 'Weighted Average Weekly Sales' by multiplying 'Average Weekly Sales' by 'weekly_sales_rate' when 'Stockout Days in Week' is 7\n",
    "df_weekly['Weighted Average Weekly Sales'] = df_weekly['Average Weekly Sales'] * df_weekly['weekly_sales_rate'] * (df_weekly['Stockout Days in Week'] == 7)\n",
    "# Round up the values in the 'Weighted Average Weekly Sales' column\n",
    "df_weekly['Weighted Average Weekly Sales'] = np.ceil(df_weekly['Weighted Average Weekly Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill the 'Adjusted Sales' column with the values from the 'Weighted Average Weekly Sales' column where 'Stockout Days in Week' is 7\n",
    "df_weekly['Adjusted Sales'] = np.where(df_weekly['Stockout Days in Week'] == 7, df_weekly['Weighted Average Weekly Sales'], df_weekly['Adjusted Sales'])\n",
    "df_weekly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summation of adjusted sales and sales qty total for fiscal year 2022 and 2023 \n",
    "adjusted_sales_total = df_weekly['Adjusted Sales'].sum()\n",
    "sales_qty_total = df_weekly['Sales Qty'].sum()\n",
    "\n",
    "adjusted_sales_total, sales_qty_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lost sales inter/intra weekly\n",
    "lost_sales = adjusted_sales_total - sales_qty_total\n",
    "lost_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size- Demand Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_size = pd.read_excel(\"Outlet X cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size weight mappings based on Gender and Sizing Type.\n",
    "size_weights = {\n",
    "    ('Men', 'sml'): {'L': 0.34, 'M': 0.38, 'S': 0.09, 'XL': 0.17, 'XXL': 0.02},\n",
    "    ('Women', 'sml'): {'L': 0.19, 'M': 0.37, 'S': 0.34, 'XS': 0.10},\n",
    "    ('Women', 'brasize'): {\n",
    "        '32 0A': 0.03, '32 0B': 0.03, '32 0C': 0.04, '32 0D': 0.02, \n",
    "        '34 0A': 0.07, '34 0B': 0.13, '34 0C': 0.13, '34 0D': 0.08, \n",
    "        '36 0A': 0.04, '36 0B': 0.10, '36 0C': 0.09, '36 0D': 0.07,\n",
    "        '38 0B': 0.05, '38 0C': 0.07, '38 0D': 0.05\n",
    "    }\n",
    "}\n",
    "\n",
    "def assign_size_weight(row):\n",
    "    # Extract gender, sizing type, and article size from the row\n",
    "    gender = row['Gender']\n",
    "    sizing_type = row['Sizing Type']\n",
    "    article_size = row['Article: Size ']\n",
    "    \n",
    "    # Fetch the weight using the tuple key (Gender, Sizing Type) and the size as the inner key.\n",
    "    return size_weights.get((gender, sizing_type), {}).get(article_size, None)\n",
    "\n",
    "# Apply the function across each row to create the new 'Size weight' column\n",
    "df_size['Size weight'] = df_size.apply(assign_size_weight, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now adding the 'Weighted Inventory Size' column\n",
    "df_size['Weighted Inventory Size'] = df_size['Inv_yn'] * df_size['Size weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table that sums up the 'Weighted Inventory Size' and 'Sales Qty' for each 'Article: Generic ' and 'Calendar Day'\n",
    "pivot_table = pd.pivot_table(df_size, values=['Weighted Inventory Size', 'Sales Qty'], index=['Article: Generic ', 'Calendar Day'], aggfunc=np.sum)\n",
    "\n",
    "# Reset the index of the pivot table to make it easier to merge\n",
    "pivot_table.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns of the pivot table to make them easier to distinguish\n",
    "pivot_table.rename(columns={'Weighted Inventory Size': 'Weighted Inventory Size_sum', 'Sales Qty': 'Sales Qty_total'}, inplace=True)\n",
    "\n",
    "# Merge the pivot table with the original DataFrame\n",
    "df_size = pd.merge(df_size, pivot_table, on=['Article: Generic ', 'Calendar Day'])\n",
    "\n",
    "# Create the 'Weights' column\n",
    "df_size['Weights'] = np.where(df_size['Inv_yn'] == 1, 0, df_size['Weighted Inventory Size_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'inf' values with 0 in the 'Additional Sales' column\n",
    "df_size['Additional Sales'] = np.ceil((df_size['Sales Qty_total'] / df_size['Weights']) * df_size['Size weight'])\n",
    "df_size['Additional Sales'] = df_size['Additional Sales'].replace(np.inf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'Adjusted Sales Size' column\n",
    "df_size['Adjusted Sales Size'] = df_size['Sales Qty'] + df_size['Additional Sales']\n",
    "\n",
    "#fill the empty values with 0\n",
    "df_size['Adjusted Sales Size'] = df_size['Adjusted Sales Size'].fillna(0)\n",
    "df_size.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the sum of adjusted sales and sales qty total\n",
    "adjusted_sales_total = df_size['Adjusted Sales Size'].sum()\n",
    "sales_qty_total = df_size['Sales Qty'].sum()\n",
    "\n",
    "adjusted_sales_total, sales_qty_total \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly_size = df_size.groupby(['Article', 'Fiscal Year', 'Fiscal Week']).agg({\n",
    "    'FMS Site': 'first',\n",
    "    'FMS Site Name': 'first',\n",
    "    'Gender': 'first',\n",
    "    'Article: MH5 Product Group': 'first',\n",
    "    'Article: MH6 Sub Product Group': 'first',\n",
    "    'Article: Generic ': 'first',\n",
    "    'Article: Generic Name': 'first',\n",
    "    'Article: Color Description': 'first',\n",
    "    'Article Name': 'first',\n",
    "    'Article: Size ': 'first',\n",
    "    'Sales Qty': 'sum',\n",
    "    'Size weight': 'first',\n",
    "    'Weighted Inventory Size': 'sum',\n",
    "    'Opening Balance Qty': 'first',\n",
    "    'Delta In Transit Qty': 'sum',\n",
    "    'Closing Balance Qty': 'last',\n",
    "    'Stockout Days in Week': 'first',\n",
    "    'Adjusted Sales Size': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "df_weekly_size.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company's current method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us = df.groupby(['Article', 'Fiscal Year', 'Fiscal Week']).agg({\n",
    "    'Fiscal Year Week': 'first',\n",
    "    'FMS Site': 'first',\n",
    "    'FMS Site Name': 'first',\n",
    "    'Gender': 'first',\n",
    "    'Article: MH5 Product Group': 'first',\n",
    "    'Article: MH6 Sub Product Group': 'first',\n",
    "    'Article: Generic ': 'first',\n",
    "    'Article: Generic Name': 'first',\n",
    "    'Article: Color Description': 'first',\n",
    "    'Article Name': 'first',\n",
    "    'Article: Size ': 'first',\n",
    "    'Sales Qty': 'sum',\n",
    "    'Closing Balance Qty': 'last',\n",
    "   \n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.sort_values(by=['Article', 'Fiscal Year', 'Fiscal Week'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_data = df_us.groupby(['Article', 'Fiscal Year', 'Fiscal Week', 'FMS Site', 'Article: Generic ','Article: Generic Name', 'Article: Color Description', 'Article Name', 'Article: Size '])\\\n",
    "              .agg(Units=('Sales Qty', 'sum'),\n",
    "                   Inv=('Closing Balance Qty', 'sum')).reset_index()\n",
    "\n",
    "# Calculate days with positive inventory (denominator for sales rate)\n",
    "# Assume that negative or 0 inventory at the end of the week implies that inventory ran out the middle of the week\n",
    "sr_data['Inv.Days'] = sr_data['Inv'].apply(lambda x: 7 if x > 0 else 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by 'Fiscal Year', 'Fiscal Week' to ensure the cumulative sum is calculated correctly\n",
    "sr_data.sort_values(['Article', 'Fiscal Year', 'Fiscal Week'], inplace=True)\n",
    "\n",
    "# Calculate the cumulative sales by week by year for each article\n",
    "sr_data['Cumulative Units'] = sr_data.groupby(['Article'])['Units'].cumsum()\n",
    "sr_data['Cumulative Inv.Days'] = sr_data.groupby(['Article'])['Inv.Days'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative sales rate as the ceiling of the ratio of cumulative sums multiplied by 7\n",
    "sr_data['Sales.Rate'] = np.ceil((sr_data['Cumulative Units'] / sr_data['Cumulative Inv.Days']) * 7)\n",
    "sr_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'bop.inv' column\n",
    "sr_data['bop.inv'] = sr_data.groupby(['Article'])['Inv'].shift(1)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "sr_data['bop.inv'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'Lost.Sales'\n",
    "conditions = [\n",
    "    (sr_data['bop.inv'] <= 0) & (sr_data['Inv'] <= 0),  # BOP & EOP == 0\n",
    "    ((sr_data['bop.inv'] == 0) | (sr_data['Inv'] == 0)) & ((sr_data['bop.inv'] + sr_data['Inv']) > 0),  # BOP | EOP == 0\n",
    "    (sr_data['bop.inv'] > 0) & (sr_data['Inv'] > 0)  # BOP & EOP > 0\n",
    "]\n",
    "choices = [\n",
    "    sr_data['Sales.Rate'],  # Lost.Sales = Sales.Rate\n",
    "    sr_data['Sales.Rate'] / 2,  # Lost.Sales = Sales.Rate / 2\n",
    "    0  # Lost.Sales = 0\n",
    "]\n",
    "sr_data['Lost.Sales'] = np.select(conditions, choices, default=0)\n",
    "\n",
    "# Round 'Lost.Sales' to 0 decimal places\n",
    "sr_data['Lost.Sales'] = sr_data['Lost.Sales'].round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Adjusted Sales' column\n",
    "sr_data['Adjusted Sales'] = sr_data['Lost.Sales'] + sr_data['Units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the sum of adjusted sales and sales qty total\n",
    "adjusted_sales_total = sr_data['Adjusted Sales'].sum()\n",
    "sales_qty_total = sr_data['Units'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SImulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating stockouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot histograms and box plots for stockout days in 2022 and 2023\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_weekly['Stockout Days in Week'], kde=True, color='blue', bins=7)\n",
    "plt.title('2022 Stockout Days Distribution')\n",
    "plt.xlabel('Stockout Days in Week')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_weekly['Stockout Days in Week'], kde=True, color='green', bins=7)\n",
    "plt.title('2023 Stockout Days Distribution')\n",
    "plt.xlabel('Stockout Days in Week')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import multinomial\n",
    "\n",
    "# Assuming df_weekly_stockouts is already defined\n",
    "stockout_days = df_weekly_stockouts['Stockout Days in Week'].values\n",
    "\n",
    "# Filter out zero stockout days\n",
    "stockout_days = stockout_days[stockout_days > 0]\n",
    "\n",
    "# Calculate the observed frequencies of each stockout day\n",
    "unique, counts = np.unique(stockout_days, return_counts=True)\n",
    "\n",
    "# Total number of observations\n",
    "n = len(stockout_days)\n",
    "\n",
    "# Calculate probabilities for the observed stockout days\n",
    "probabilities = counts / n\n",
    "\n",
    "# Plotting the observed frequencies and the multinomial PMF\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(stockout_days, kde=False, bins=range(1, max(stockout_days) + 2), color='g', label='Stockout Data', stat='density')\n",
    "\n",
    "# Plot the observed probabilities\n",
    "plt.plot(unique, probabilities, 'ro', ms=8, label='Observed Probabilities')\n",
    "plt.title('Histogram of Stockout Days in a Week with Observed Probabilities')\n",
    "plt.xlabel('Number of Stockout Days')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_needed = ['Calendar Day', 'Fiscal Year Week', 'Fiscal Year','Fiscal Week', 'FMS Site','Gender', 'Article: MH5 Product Group', 'Article: MH6 Sub Product Group','Article: Generic ', 'Article: Generic Name', 'Article: Color Description', 'Article', 'Article Name', 'Article: Size ', 'Opening Balance Qty','Sales Qty', 'Closing Balance Qty', 'Inv_yn', 'Stockout1', 'Stockout Days in Week', 'Sales Weight']\n",
    "\n",
    "# Create a new DataFrame with only these columns\n",
    "df_needed = df[columns_needed]\n",
    "\n",
    "# Create a copy of the 'Sales Qty' column for comparison later\n",
    "df_needed['Original Sales Qty'] = df_needed['Sales Qty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for size to demand relationship accuracy\n",
    "df_needed1 = df_needed.copy()\n",
    "df_needed1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for inter and intra week cyclicility\n",
    "# Merge average weekly sales into df_needed\n",
    "df_needed = pd.merge(df_needed, average_weekly_sales, on=['Article', 'Article Name', 'Fiscal Year'], how='left')\n",
    "\n",
    "# Merge weekly sales rate into df_needed\n",
    "df_needed = pd.merge(df_needed, merged_df[['Article: MH5 Product Group', 'Fiscal Year', 'Fiscal Week', 'weekly_sales_rate']], \n",
    "                     on=['Article: MH5 Product Group', 'Fiscal Year', 'Fiscal Week'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the sales weight column from df_needed1\n",
    "df_needed1.drop(columns='Sales Weight', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needed1['Sizing Type']= df_needed1['Article: MH6 Sub Product Group'].map({\n",
    "'Men Boxers Briefs': 'sml',\n",
    "' Men Hip Briefs': 'sml',\n",
    "'Men Low Rise Trunks': 'sml',\n",
    "'Men High Rise Trunks': 'sml',\n",
    "'Women Coordinate Boy Shorts': 'sml',\n",
    "'Women Coordinate Briefs': 'sml',\n",
    "'Women Coordinate Thongs':'sml',\n",
    "'Women Panty Table Briefs': 'sml',\n",
    "'Women Panty Table Hipsters': 'sml',\n",
    "'Women Panty Table Thongs':'sml',\n",
    "'Women Bralette Bras': 'sml',\n",
    "'Women Other Bras': 'brasize',\n",
    "})\n",
    "\n",
    "df_needed1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_weights = {\n",
    "    ('Men', 'sml'): {'L': 0.34, 'M': 0.38, 'S': 0.09, 'XL': 0.17, 'XXL': 0.02},\n",
    "    ('Women', 'sml'): {'L': 0.19, 'M': 0.37, 'S': 0.34, 'XS': 0.10},\n",
    "    ('Women', 'brasize'): {\n",
    "        '32 0A': 0.03, '32 0B': 0.03, '32 0C': 0.04, '32 0D': 0.02, \n",
    "        '34 0A': 0.07, '34 0B': 0.13, '34 0C': 0.13, '34 0D': 0.08, \n",
    "        '36 0A': 0.04, '36 0B': 0.10, '36 0C': 0.09, '36 0D': 0.07,\n",
    "        '38 0B': 0.05, '38 0C': 0.07, '38 0D': 0.05\n",
    "    }\n",
    "}\n",
    "def assign_size_weight(row):\n",
    "    # Extract gender, sizing type, and article size from the row\n",
    "    gender = row['Gender']\n",
    "    sizing_type = row['Sizing Type']\n",
    "    article_size = row['Article: Size ']\n",
    "    \n",
    "    # Fetch the weight using the tuple key (Gender, Sizing Type) and the size as the inner key.\n",
    "    return size_weights.get((gender, sizing_type), {}).get(article_size, None)\n",
    "\n",
    "# Apply the function across each row to create the new 'Size weight' column\n",
    "df_needed1['Size weight'] = df_needed1.apply(assign_size_weight, axis=1)\n",
    "df_needed1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intra-Inter Week Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rando\n",
    "\n",
    "def run_simulation(df_needed, iterations, probabilities, seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    results = []\n",
    "    wmape_list = []\n",
    "    wmpe_list = []\n",
    "    df_copy_list = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        df_copy = df_needed.copy()\n",
    "        df_copy['Simulated Stockout Days in Week'] = 0\n",
    "\n",
    "        # Filter out weeks where the articles were in stock for the entire week\n",
    "        df_copy = df_copy[df_copy['Stockout Days in Week'] == 0]\n",
    "\n",
    "        # For each article, year, and week, simulate stockout days using Multinomial distribution\n",
    "        weekly_groups = df_copy.groupby(['Article', 'Fiscal Year', 'Fiscal Week'])\n",
    "        for (article, year, week), group in weekly_groups:\n",
    "            # Draw number of stockout days using multinomial distribution\n",
    "            num_days_to_stockout = np.argmax(np.random.multinomial(1, probabilities)) + 1\n",
    "            daily_indices = list(group.index)\n",
    "            if len(daily_indices) >= num_days_to_stockout:\n",
    "                simulated_stockout_indices = random.sample(daily_indices, num_days_to_stockout)\n",
    "                df_copy.loc[simulated_stockout_indices, 'Inv_yn'] = 0\n",
    "                df_copy.loc[simulated_stockout_indices, 'Sales Qty'] = 0\n",
    "                df_copy.loc[simulated_stockout_indices, 'Simulated Stockout Days in Week'] += 1\n",
    "\n",
    "        # Calculate weighted inventory for intra-week cyclicity\n",
    "        df_copy['Weighted Inventory'] = df_copy['Inv_yn'] * df_copy['Sales Weight']\n",
    "\n",
    "        # Aggregate the data to weekly-level DataFrame\n",
    "        df_weekly1 = df_copy.groupby(['Article', 'Fiscal Year', 'Fiscal Week']).agg({\n",
    "            'FMS Site': 'first',\n",
    "            'Sales Qty': 'sum',\n",
    "            'Article: MH5 Product Group': 'first',\n",
    "            'Article: MH6 Sub Product Group': 'first',\n",
    "            'Article: Generic ': 'first',\n",
    "            'Article: Generic Name': 'first',\n",
    "            'Article: Color Description': 'first',\n",
    "            'Original Sales Qty': 'sum',\n",
    "            'Weighted Inventory': 'sum',\n",
    "            'Inv_yn': 'last',\n",
    "            'Stockout Days in Week': 'first',\n",
    "            'Simulated Stockout Days in Week': 'sum',\n",
    "            'Average Weekly Sales': 'first',\n",
    "            'weekly_sales_rate': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Apply adjustments for intra-week and inter-week cyclicity\n",
    "        df_weekly1['Adjusted Sales'] = df_weekly1.apply(\n",
    "            lambda row: (row['Average Weekly Sales'] * row['weekly_sales_rate']\n",
    "                         if row['Simulated Stockout Days in Week'] == 7\n",
    "                         else (row['Sales Qty'] / row['Weighted Inventory']) * 100),\n",
    "            axis=1\n",
    "        )\n",
    "        df_weekly1['Adjusted Sales'] = df_weekly1['Adjusted Sales'].round(0)\n",
    "        df_weekly1['Difference'] = df_weekly1['Adjusted Sales'] - df_weekly1['Original Sales Qty']\n",
    "\n",
    "        # Calculate error metrics\n",
    "        mae = np.mean(np.abs(df_weekly1['Difference']))\n",
    "        rmse = np.sqrt(np.mean(df_weekly1['Difference']**2))\n",
    "        mape = np.mean(np.abs(df_weekly1['Difference'] / df_weekly1['Original Sales Qty'])) * 100\n",
    "        wmape = np.sum(np.abs(df_weekly1['Difference'])) / np.sum(df_weekly1['Original Sales Qty']) * 100\n",
    "        wmpe = np.sum(df_weekly1['Difference']) / np.sum(df_weekly1['Original Sales Qty']) * 100\n",
    "\n",
    "        results.append(df_weekly1['Difference'].mean())\n",
    "        wmape_list.append(wmape)\n",
    "        wmpe_list.append(wmpe)\n",
    "        df_copy_list.append(df_copy)\n",
    "\n",
    "    return results, mae_list, rmse_list, mape_list, wmape_list, wmpe_list, df_weekly1, df_copy_list\n",
    "\n",
    "# Probabilities for the multinomial distribution\n",
    "probabilities = [0.16, 0.10, 0.08, 0.06, 0.10, 0.02, 0.48]\n",
    "seed_value = 12345\n",
    "\n",
    "results, wmape_list, wmpe_list, df_weekly1, df_copy_list = run_simulation(df_needed, 100, probabilities, seed_value)\n",
    "\n",
    "# Display results\n",
    "average_difference = np.mean(results)\n",
    "std_dev_difference = np.std(results)\n",
    "average_wmape = np.mean(wmape_list)\n",
    "average_wmpe = np.mean(wmpe_list)\n",
    "\n",
    "print(f'Average Difference: {average_difference}')\n",
    "print(f'Standard Deviation of Difference: {std_dev_difference}')\n",
    "print(f'Average wMAPE: {average_wmape}')\n",
    "print(f'Average wMPE: {average_wmpe}')\n",
    "\n",
    "# Access df_copy_list for the last iteration's daily-level data\n",
    "final_df_copy = df_copy_list[-1]\n",
    "final_df_copy.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def run_simulation(df_needed, iterations, probabilities, seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    results = []\n",
    "    wmape_list = []\n",
    "    wmpe_list = []\n",
    "    df_copy_list = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        df_copy = df_needed.copy()\n",
    "        df_copy['Simulated Stockout Days in Week'] = 0\n",
    "\n",
    "        # Filter out articles that were in stock for the entire week\n",
    "        df_copy = df_copy[df_copy['Stockout Days in Week'] == 0]\n",
    "\n",
    "        # For each article, year, and week, simulate stockout days using Multinomial distribution\n",
    "        weekly_groups = df_copy.groupby(['Article', 'Fiscal Year', 'Fiscal Week'])\n",
    "        for (article, year, week), group in weekly_groups:\n",
    "            # Draw number of stockout days using multinomial distribution\n",
    "            num_days_to_stockout = np.argmax(np.random.multinomial(1, probabilities)) + 1\n",
    "            daily_indices = list(group.index)\n",
    "            if len(daily_indices) >= num_days_to_stockout:\n",
    "                simulated_stockout_indices = random.sample(daily_indices, num_days_to_stockout)\n",
    "                df_copy.loc[simulated_stockout_indices, 'Inv_yn'] = 0\n",
    "                df_copy.loc[simulated_stockout_indices, 'Sales Qty'] = 0\n",
    "                df_copy.loc[simulated_stockout_indices, 'Simulated Stockout Days in Week'] += 1\n",
    "\n",
    "        # Calculate weighted inventory for intra-week cyclicity\n",
    "        df_copy['Weighted Inventory'] = df_copy['Inv_yn'] * df_copy['Sales Weight']\n",
    "\n",
    "        # Aggregate the data to weekly-level DataFrame\n",
    "        df_weekly1 = df_copy.groupby(['Article', 'Fiscal Year', 'Fiscal Week']).agg({\n",
    "            'FMS Site': 'first',\n",
    "            'Sales Qty': 'sum',\n",
    "            'Article: MH5 Product Group': 'first',\n",
    "            'Article: MH6 Sub Product Group': 'first',\n",
    "            'Article: Generic ': 'first',\n",
    "            'Article: Generic Name': 'first',\n",
    "            'Article: Color Description': 'first',\n",
    "            'Original Sales Qty': 'sum',\n",
    "            'Weighted Inventory': 'sum',\n",
    "            'Inv_yn': 'last',\n",
    "            'Stockout Days in Week': 'first',\n",
    "            'Simulated Stockout Days in Week': 'sum',\n",
    "            'Average Weekly Sales': 'first',\n",
    "            'weekly_sales_rate': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Apply adjustments for intra-week and inter-week cyclicity\n",
    "        df_weekly1.rename(columns={'Inv_yn': 'Inventory Closing'}, inplace=True)\n",
    "        df_weekly1['Inv.days'] = np.where(df_weekly1['Inventory Closing'] == 0, 3.5, 7)\n",
    "        df_weekly1.sort_values(['Article', 'Fiscal Year', 'Fiscal Week'], inplace=True)\n",
    "\n",
    "        # Calculate the cumulative sales by week by year for each article\n",
    "        df_weekly1['Cumulative Sales'] = df_weekly1.groupby('Article')['Sales Qty'].cumsum()\n",
    "        df_weekly1['Cumulative Inv.days'] = df_weekly1.groupby('Article')['Inv.days'].cumsum()\n",
    "        \n",
    "        # Calculate the cumulative sales rate as the ceiling of the ratio of cumulative sums multiplied by 7\n",
    "        df_weekly1['Sales Rate'] = np.ceil(df_weekly1['Cumulative Sales'] / df_weekly1['Cumulative Inv.days'] * 7)\n",
    "        df_weekly1['Sales Rate'].fillna(0, inplace=True)\n",
    "        \n",
    "        # Create the 'bop.inv' column and fill the \n",
    "        df_weekly1['bop.inv'] = df_weekly1.groupby('Article')['Inventory Closing'].shift(1)\n",
    "        df_weekly1['bop.inv'].fillna(0, inplace=True)\n",
    "        \n",
    "        #calculate the lost sales conditions \n",
    "        conditions= [\n",
    "            (df_weekly1['bop.inv'] == 0) & (df_weekly1['Inventory Closing'] == 0), #BOP and closing inventory =0 \n",
    "            # BOP or closing inventory =0 either one of them is zero\n",
    "            (df_weekly1['bop.inv'] == 0) | (df_weekly1['Inventory Closing'] == 0),\n",
    "            #BOP and colosing =1\n",
    "            (df_weekly1['bop.inv'] == 1) & (df_weekly1['Inventory Closing'] == 1), \n",
    "            ]                   \n",
    "        # in first condition the lost sales is the sales rate\n",
    "        #in secoond condition lost sales is sales rate/2\n",
    "        #in third condition lost sales is 0\n",
    "        choices =[\n",
    "            df_weekly1['Sales Rate'],\n",
    "            df_weekly1['Sales Rate']/2,\n",
    "            0\n",
    "        ]\n",
    "\n",
    "        df_weekly1['Lost Sales'] = np.select(conditions, choices, default=0)\n",
    "\n",
    "        #round the lost sales to 0 decimal places\n",
    "        df_weekly1['Lost Sales'] = df_weekly1['Lost Sales'].round(0)\n",
    "\n",
    "        df_weekly1.head()\n",
    "\n",
    "        df_weekly1['Adjusted Sales'] = df_weekly1['Sales Qty'] + df_weekly1['Lost Sales']\n",
    "        df_weekly1['Adjusted Sales'] = df_weekly1['Adjusted Sales'].round(0)\n",
    "        \n",
    "        df_weekly1['Difference'] = df_weekly1['Adjusted Sales'] - df_weekly1['Original Sales Qty']\n",
    "        # Calculate error metrics\n",
    "        wmape = np.sum(np.abs(df_weekly1['Difference'])) / np.sum(df_weekly1['Original Sales Qty']) * 100\n",
    "        wmpe = np.sum(df_weekly1['Difference']) / np.sum(df_weekly1['Original Sales Qty']) * 100\n",
    "\n",
    "        results.append(df_weekly1['Difference'].mean())\n",
    "        wmape_list.append(wmape)\n",
    "        wmpe_list.append(wmpe)\n",
    "        df_copy_list.append(df_copy)\n",
    "\n",
    "    return results, wmape_list, wmpe_list, df_weekly1, df_copy_list\n",
    "\n",
    "# Set the probabilities for the multinomial distribution\n",
    "probabilities = [0.16, 0.10, 0.08, 0.06, 0.10, 0.02, 0.48]\n",
    "seed_value = 12345\n",
    "\n",
    "results, wmape_list, wmpe_list, df_weekly1, df_copy_list = run_simulation(df_needed, 100, probabilities, seed_value)\n",
    "\n",
    "# Display results\n",
    "average_difference = np.mean(results)\n",
    "std_dev_difference = np.std(results)\n",
    "average_wmape = np.mean(wmape_list)\n",
    "average_wmpe = np.mean(wmpe_list)\n",
    "\n",
    "print(f'Average Difference: {average_difference}')\n",
    "print(f'Standard Deviation of Difference: {std_dev_difference}')\n",
    "print(f'Average wMAPE: {average_wmape}')\n",
    "print(f'Average wMPE: {average_wmpe}')\n",
    "\n",
    "# Access df_copy_list for the last iteration's daily-level data\n",
    "final_df_copy = df_copy_list[-1]\n",
    "final_df_copy.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size Demand Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def run_simulation(df_needed1, iterations, probabilities, seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    results = []\n",
    "    wmape_list = []\n",
    "    wmpe_list = []\n",
    "    df_size_list = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        df_size1 = df_needed1.copy()\n",
    "        df_size1['Simulated Stockout Days in Week'] = 0\n",
    "\n",
    "        # Filter out articles that were in stock for the entire week\n",
    "        df_size1 = df_size1[df_size1['Stockout Days in Week'] == 0]\n",
    "\n",
    "        # For each article, year, and week, simulate stockout days using Multinomial distribution\n",
    "        weekly_groups = df_size1.groupby(['Article', 'Fiscal Year', 'Fiscal Week'])\n",
    "        for (article, year, week), group in weekly_groups:\n",
    "            # Draw number of stockout days using multinomial distribution\n",
    "            num_days_to_stockout = np.argmax(np.random.multinomial(1, probabilities)) + 1\n",
    "            daily_indices = list(group.index)\n",
    "            if len(daily_indices) >= num_days_to_stockout:\n",
    "                simulated_stockout_indices = random.sample(daily_indices, num_days_to_stockout)\n",
    "                df_size1.loc[simulated_stockout_indices, 'Inv_yn'] = 0\n",
    "                df_size1.loc[simulated_stockout_indices, 'Sales Qty'] = 0\n",
    "                df_size1.loc[simulated_stockout_indices, 'Simulated Stockout Days in Week'] += 1\n",
    "\n",
    "        # Calculate weighted inventory for intra-week cyclicity\n",
    "        df_size1['Weighted Inventory Size'] = df_size1['Inv_yn'] * df_size1['Size weight']\n",
    "        \n",
    "        grouped = df_size1.groupby(['Article: Generic ', 'Calendar Day']).agg({\n",
    "        'Weighted Inventory Size': 'sum',\n",
    "        'Sales Qty': 'sum'\n",
    "                    }).reset_index()\n",
    "        \n",
    "        grouped.rename(columns={\n",
    "            'Weighted Inventory Size': 'Weighted Inventory Size_sum',\n",
    "            'Sales Qty': 'Sales Qty_total'\n",
    "                }, inplace=True)\n",
    "\n",
    "        df_size1 = pd.merge(df_size1, grouped, on=['Article: Generic ', 'Calendar Day'], how='left')\n",
    "        \n",
    "        df_size1['Weights'] = np.where(df_size1['Inv_yn'] == 1, 0, df_size1['Weighted Inventory Size_sum'])\n",
    "\n",
    "        df_size1['Additional Sales'] = np.ceil((df_size1['Sales Qty_total'] / df_size1['Weights']) * df_size1['Size weight'])\n",
    "        df_size1['Additional Sales'] = df_size1['Additional Sales'].replace(np.inf, 0)\n",
    "        df_size1['Adjusted Sales Size'] = df_size1['Sales Qty'] + df_size1['Additional Sales']\n",
    "\n",
    "\n",
    "        # Create the weekly-level DataFrame by aggregating data\n",
    "        df_weekly_size1 = df_size1.groupby(['Article', 'Fiscal Year', 'Fiscal Week']).agg({\n",
    "            'FMS Site': 'first',\n",
    "            'Article: MH5 Product Group': 'first',\n",
    "            'Article: MH6 Sub Product Group': 'first',\n",
    "            'Article: Generic ': 'first',\n",
    "            'Article: Generic Name': 'first',\n",
    "            'Article: Color Description': 'first',\n",
    "            'Article Name': 'first',\n",
    "            'Article: Size ': 'first',\n",
    "            'Sales Qty': 'sum',\n",
    "            'Original Sales Qty': 'sum',\n",
    "            'Size weight': 'first',\n",
    "            'Weighted Inventory Size': 'sum',\n",
    "            'Stockout Days in Week': 'first',\n",
    "            'Simulated Stockout Days in Week': 'sum',\n",
    "            'Adjusted Sales Size': 'sum'\n",
    "        \n",
    "        }).reset_index()\n",
    "        \n",
    "        df_weekly_size1['Difference'] = df_weekly_size1['Adjusted Sales Size'] - df_weekly_size1['Original Sales Qty']\n",
    "\n",
    "        # Calculate error metrics\n",
    "        mae = np.mean(np.abs(df_weekly_size1['Difference']))\n",
    "        rmse = np.sqrt(np.mean(df_weekly_size1['Difference']**2))\n",
    "        mape = np.mean(np.abs(df_weekly_size1['Difference'] / df_weekly_size1['Original Sales Qty'])) * 100\n",
    "        wmape = np.sum(np.abs(df_weekly_size1['Difference'])) / np.sum(df_weekly_size1['Original Sales Qty']) * 100\n",
    "        wmpe = np.sum(df_weekly_size1['Difference']) / np.sum(df_weekly_size1['Original Sales Qty']) * 100\n",
    "        \n",
    "        results.append(df_weekly_size1['Difference'].mean())\n",
    "        mae_list.append(mae)\n",
    "        rmse_list.append(rmse)\n",
    "        mape_list.append(mape)\n",
    "        wmape_list.append(wmape)\n",
    "        wmpe_list.append(wmpe)\n",
    "        df_size_list.append(df_size1)\n",
    "\n",
    "    return results, mae_list, rmse_list, mape_list, wmape_list, wmpe_list, df_weekly_size1, df_size_list\n",
    "\n",
    "# Set the probabilities for the multinomial distribution\n",
    "probabilities = [0.16, 0.10, 0.08, 0.06, 0.10, 0.02, 0.48]\n",
    "seed_value = 12345\n",
    "\n",
    "results, wmape_list, wmpe_list, df_weekly_size1, df_size_list = run_simulation(df_needed1, 10, probabilities, seed_value)\n",
    "\n",
    "# Display results\n",
    "average_difference = np.mean(results)\n",
    "std_dev_difference = np.std(results)\n",
    "average_wmape = np.mean(wmape_list)\n",
    "average_wmpe = np.mean(wmpe_list)\n",
    "\n",
    "print(f'Average Difference: {average_difference}')\n",
    "print(f'Standard Deviation of Difference: {std_dev_difference}')\n",
    "print(f'Average wMAPE: {average_wmape}')\n",
    "print(f'Average wMPE: {average_wmpe}')\n",
    "\n",
    "# Access df_copy_list for the last iteration's daily-level data\n",
    "final_size_copy = df_size_list[-1]\n",
    "final_size_copy.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Approach 2 Simulating demand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from openpyxl import Workbook\n",
    "\n",
    "def simulate_demand(sku_sales, weekly_proportions, weekday_proportions):\n",
    "    # Ensure the proportions sum to 1\n",
    "    if not (np.isclose(sum(weekly_proportions), 1) and np.isclose(sum(weekday_proportions), 1)):\n",
    "        raise ValueError(\"Proportions must sum to 1.\")\n",
    "    \n",
    "    # Calculate total weeks in a year\n",
    "    total_weeks = 52\n",
    "    weekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    \n",
    "    # Initialize a list to store results\n",
    "    demand_simulation = []\n",
    "\n",
    "    # Iterate over each SKU\n",
    "    for index, row in sku_sales.iterrows():\n",
    "        sku = row['SKU']\n",
    "        total_annual_sales = row['Annual_Sales']\n",
    "        \n",
    "        # Generate weekly demand based on multinomial distribution\n",
    "        weekly_demand = np.random.multinomial(total_annual_sales, weekly_proportions)\n",
    "        \n",
    "        # Simulate daily demand within each week using multinomial distribution\n",
    "        for week in range(total_weeks):\n",
    "            daily_demand = np.random.multinomial(weekly_demand[week], weekday_proportions)\n",
    "            for day in range(7):\n",
    "                demand_simulation.append({'SKU': sku, 'Week': week+1, 'Day': weekdays[day], 'Demand': daily_demand[day]})\n",
    "    \n",
    "    # Convert list to DataFrame\n",
    "    demand_simulation_df = pd.DataFrame(demand_simulation)\n",
    "    \n",
    "    return demand_simulation_df\n",
    "\n",
    "# Example SKU sales data\n",
    "sku_sales = pd.DataFrame({\n",
    "    'SKU': [\n",
    "        '01', '02', '03', '04',\n",
    "        '05', '06', '07', '08',\n",
    "        '08', '09', '10', '11', '12'\n",
    "    ],\n",
    "    'Annual_Sales': [200, 180, 90, 50, 260, 260, 80, 170, 220, 190, 50, 180]\n",
    "})\n",
    "\n",
    "# Example: Custom weekly and weekday proportions\n",
    "weekly_proportions = [1/52] * 52 \n",
    "\n",
    "weekday_proportions = [0.12, 0.10, 0.10, 0.10, 0.13, 0.19, 0.26] \n",
    "\n",
    "# Simulate demand\n",
    "np.random.seed(123) # Set seed for reproducibility\n",
    "demand_simulation_df = simulate_demand(sku_sales, weekly_proportions, weekday_proportions)\n",
    "\n",
    "# View the first few rows of the simulation result\n",
    "print(demand_simulation_df.head())\n",
    "\n",
    "# Check the total demand to ensure it matches the total annual sales\n",
    "total_demand_check = demand_simulation_df.groupby('SKU')['Demand'].sum().reset_index()\n",
    "print(total_demand_check)\n",
    "\n",
    "# Aggregate demand by weekday for plotting\n",
    "weekday_demand = demand_simulation_df.groupby('Day')['Demand'].sum().reset_index()\n",
    "weekday_demand['Proportion'] = weekday_demand['Demand'] / weekday_demand['Demand'].sum()\n",
    "weekday_demand['Day'] = pd.Categorical(weekday_demand['Day'], categories=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"], ordered=True)\n",
    "weekday_demand = weekday_demand.sort_values('Day')\n",
    "\n",
    "# Plot the proportion of sales per weekday\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(weekday_demand['Day'], weekday_demand['Proportion'], color='skyblue')\n",
    "for index, value in enumerate(weekday_demand['Proportion']):\n",
    "    plt.text(index, value, f'{value:.2%}', ha='center', va='bottom')\n",
    "plt.title('Proportion of Sales per Weekday')\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('Proportion of Total Sales')\n",
    "plt.ylim(0, max(weekday_demand['Proportion']) + 0.05)\n",
    "plt.show()\n",
    "\n",
    "demand_simulation_df.to_excel(\"Sim_demand(12SKUS).xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"simulation_final.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Weekday' and calculate the total sales for each weekday\n",
    "total_sales_weekday = df.groupby('Weekday')['Sales'].sum()\n",
    "\n",
    "# Calculate the total sales for each year\n",
    "total_sales = total_sales_weekday.sum()\n",
    "\n",
    "# Calculate the sales weight for each weekday\n",
    "sales_weight_weekday = (total_sales_weekday / total_sales*100).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the average sales weight to the 'Weekday' column\n",
    "df['Sales Weight'] = df['Weekday'].map(sales_weight_weekday)\n",
    "\n",
    "#reset te index of df since it is giving weird index values\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# make a new column inv_yn which is 1 if opening inventory is greater than 0 and 0 otherwise\n",
    "df['Inv_yn'] = df['Opening Inventory'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df['Weighted Inventory'] = df['Inv_yn'] * df['Sales Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly = df.groupby(['Article', 'Fiscal Week']).agg({\n",
    "    'Article: MH5 Product Group': 'first',\n",
    "    'Article: Generic ': 'first',\n",
    "    'Sales': 'sum',\n",
    "    'Demand': 'sum',\n",
    "    'Weighted Inventory': 'sum',\n",
    "    'Opening Inventory': 'first',\n",
    "    'Replenishement Qty': 'sum',\n",
    "    'Closing Inventory': 'last',\n",
    "    'Stockout': 'sum',\n",
    "}).reset_index()\n",
    "\n",
    "#I want to multiply the adjusted sales by 100 to get the percentage of adjusted sales and want to round off the values to 0 decimal places\n",
    "df_weekly['Adjusted Sales'] = (df_weekly['Sales'] / df_weekly['Weighted Inventory']) * 100\n",
    "df_weekly['Adjusted Sales'] = df_weekly['Adjusted Sales'].round(0)\n",
    "df_weekly = df_weekly.sort_values(by=['Article', 'Fiscal Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Article: MH5 Product Group', 'Fiscal Year', and 'Fiscal Week' and calculate the total sales for each product group\n",
    "total_sales_product_group = df.groupby(['Article: MH5 Product Group', 'Fiscal Week'])['Sales'].sum()\n",
    "\n",
    "# Reset the index to make the DataFrame easier to work with\n",
    "total_sales_product_group = total_sales_product_group.reset_index()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "total_sales_product_group.head()\n",
    "\n",
    "average_sales_product_group = total_sales_product_group.groupby(['Article: MH5 Product Group'])['Sales'].mean()\n",
    "merged_df = pd.merge(total_sales_product_group, average_sales_product_group, on=['Article: MH5 Product Group'], suffixes=('_total', '_average'))\n",
    "\n",
    "# Compute the weekly sales rate\n",
    "merged_df['weekly_sales_rate'] = merged_df['Sales_total'] / merged_df['Sales_average']\n",
    "\n",
    "# Round the 'weekly_sales_rate' to 1 decimal place\n",
    "merged_df['weekly_sales_rate'] = merged_df['weekly_sales_rate'].round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_stockouts = df_weekly[df_weekly['Stockout'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Article', 'Article Name', 'Fiscal Year', and 'Fiscal Week', then calculate the mean of 'Sales Qty'\n",
    "average_weekly_sales = df_no_stockouts.groupby(['Article'])['Sales'].mean()\n",
    "\n",
    "# Reset the index to make the DataFrame easier to work with\n",
    "average_weekly_sales = average_weekly_sales.reset_index()\n",
    "\n",
    "# Rename the 'Sales Qty' column to 'Average Weekly Sales'\n",
    "average_weekly_sales.rename(columns={'Sales': 'Average Weekly Sales'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly = pd.merge(df_weekly, average_weekly_sales, how='left', on=['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly = pd.merge(df_weekly, merged_df[['Article: MH5 Product Group', 'Fiscal Week', 'weekly_sales_rate']], how='left', on=['Article: MH5 Product Group', 'Fiscal Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly['Weighted Average Weekly Sales'] = df_weekly['Average Weekly Sales'] * df_weekly['weekly_sales_rate'] * (df_weekly['Stockout'] == 7)\n",
    "\n",
    "# Round up the values in the 'Weighted Average Weekly Sales' column\n",
    "df_weekly['Weighted Average Weekly Sales'] = np.ceil(df_weekly['Weighted Average Weekly Sales'])\n",
    "df_weekly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly['Adjusted Sales'] = np.where(df_weekly['Stockout'] == 7, df_weekly['Weighted Average Weekly Sales'], df_weekly['Adjusted Sales'])\n",
    "df_weekly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_weights = {\n",
    "    'S': 0.1,\n",
    "    'M': 0.41,\n",
    "    'L': 0.34,\n",
    "    'XL': 0.15\n",
    "}\n",
    "df['Size weight'] = df['Article: Size '].map(size_weights)\n",
    "df_size['Weighted Inventory Size'] = df_size['Inv_yn'] * df_size['Size weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table that sums up the 'Weighted Inventory Size' and 'Sales Qty' for each 'Article: Generic ' and 'Calendar Day'\n",
    "pivot_table = pd.pivot_table(df_size, values=['Weighted Inventory Size', 'Sales'], index=['Article: Generic ', 'Calendar Day'], aggfunc=np.sum)\n",
    "\n",
    "# Reset the index of the pivot table to make it easier to merge\n",
    "pivot_table.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns of the pivot table to make them easier to distinguish\n",
    "pivot_table.rename(columns={'Weighted Inventory Size': 'Weighted Inventory Size_sum', 'Sales': 'Sales_total'}, inplace=True)\n",
    "\n",
    "# Merge the pivot table with the original DataFrame\n",
    "df_size = pd.merge(df_size, pivot_table, on=['Article: Generic ', 'Calendar Day'])\n",
    "\n",
    "# Create the 'Weights' column\n",
    "df_size['Weights'] = np.where(df_size['Inv_yn'] == 1, 0, df_size['Weighted Inventory Size_sum'])\n",
    "\n",
    "df_size.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'inf' values with 0 in the 'Additional Sales' column\n",
    "df_size['Additional Sales'] = np.ceil((df_size['Sales_total'] / df_size['Weights']) * df_size['Size weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'Adjusted Sales Size' column\n",
    "df_size['Adjusted Sales Size'] = df_size['Sales'] + df_size['Additional Sales']\n",
    "\n",
    "#fill the empty values with 0\n",
    "df_size['Adjusted Sales Size'] = df_size['Adjusted Sales Size'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_size_weekly= df_size.groupby(['Article', 'Fiscal Week']).agg({\n",
    "    'Article: MH5 Product Group': 'first',\n",
    "    'Article: Generic ': 'first',\n",
    "    'Sales': 'sum',\n",
    "    'Demand': 'sum',\n",
    "    'Adjusted Sales Size': 'sum',\n",
    "    'Opening Inventory': 'first',\n",
    "    'Replenishement Qty': 'sum',\n",
    "    'Closing Inventory': 'last',\n",
    "    'Stockout': 'sum',\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us = df.groupby(['Article', 'Fiscal Week']).agg({\n",
    "    'Article: MH5 Product Group': 'first',\n",
    "    'Article: Generic ': 'first',\n",
    "    'Article: Size ': 'first',\n",
    "    'Sales': 'sum',\n",
    "    'Demand': 'sum',\n",
    "    'Stockout': 'sum',\n",
    "    'Closing Inventory': 'last',\n",
    "   \n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc units and inventory by type, fiscal week, site, vendor style\n",
    "sr_data = df_us.groupby(['Article', 'Fiscal Week', 'Article: Generic ', 'Article: Size ','Demand'])\\\n",
    "              .agg(Units=('Sales', 'sum'),\n",
    "                   Inv=('Closing Inventory', 'sum'),\n",
    "                        Stockout=('Stockout','sum')).reset_index()\n",
    "\n",
    "# Calculate days with positive inventory (denominator for sales rate)\n",
    "# Assume that negative or 0 inventory at the end of the week implies that inventory ran out the middle of the week\n",
    "sr_data['Inv.Days'] = sr_data['Inv'].apply(lambda x: 7 if x > 0 else 3.5)\n",
    "\n",
    "sr_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by 'Fiscal Year', 'Fiscal Week' to ensure the cumulative sum is calculated correctly\n",
    "sr_data.sort_values(['Article', 'Fiscal Week'], inplace=True)\n",
    "\n",
    "# Calculate the cumulative sales by week by year for each article\n",
    "sr_data['Cumulative Units'] = sr_data.groupby(['Article'])['Units'].cumsum()\n",
    "sr_data['Cumulative Inv.Days'] = sr_data.groupby(['Article'])['Inv.Days'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative sales rate as the ceiling of the ratio of cumulative sums multiplied by 7\n",
    "sr_data['Sales.Rate'] = np.ceil((sr_data['Cumulative Units'] / sr_data['Cumulative Inv.Days']) * 7)\n",
    "sr_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'bop.inv' column\n",
    "sr_data['bop.inv'] = sr_data.groupby(['Article'])['Inv'].shift(1)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "sr_data['bop.inv'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'Lost.Sales'\n",
    "conditions = [\n",
    "    (sr_data['bop.inv'] <= 0) & (sr_data['Inv'] <= 0),  # BOP & EOP == 0\n",
    "    ((sr_data['bop.inv'] == 0) | (sr_data['Inv'] == 0)) & ((sr_data['bop.inv'] + sr_data['Inv']) > 0),  # BOP | EOP == 0\n",
    "    (sr_data['bop.inv'] > 0) & (sr_data['Inv'] > 0)  # BOP & EOP > 0\n",
    "]\n",
    "choices = [\n",
    "    sr_data['Sales.Rate'],  # Lost.Sales = Sales.Rate\n",
    "    sr_data['Sales.Rate'] / 2,  # Lost.Sales = Sales.Rate / 2\n",
    "    0  # Lost.Sales = 0\n",
    "]\n",
    "sr_data['Lost.Sales'] = np.select(conditions, choices, default=0)\n",
    "\n",
    "# Round 'Lost.Sales' to 0 decimal places\n",
    "sr_data['Lost.Sales'] = sr_data['Lost.Sales'].round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Adjusted Sales' column\n",
    "sr_data['Adjusted Sales'] = sr_data['Lost.Sales'] + sr_data['Units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between Adjusted sales and demand \n",
    "sr_data['Difference'] =  sr_data['Demand'] - sr_data['Adjusted Sales']\n",
    "\n",
    "wmape = np.sum(np.abs(sr_data['Difference'])) / np.sum(sr_data['Demand']) * 100\n",
    "wmpe = np.sum(sr_data['Difference']) / np.sum(sr_data['Demand']) * 100\n",
    "\n",
    "wmape, wmpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between Adjusted sales and demand \n",
    "df_weekly['Difference'] =  df_weekly['Demand'] - df_weekly['Adjusted Sales']\n",
    "\n",
    "wmape = np.sum(np.abs(df_weekly['Difference'])) / np.sum(df_weekly['Demand']) * 100\n",
    "wmpe = np.sum(df_weekly['Difference']) / np.sum(df_weekly['Demand']) * 100\n",
    "\n",
    "wmape, wmpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between Adjusted sales and demand \n",
    "df_size_weekly['Difference'] =  df_size_weekly['Demand'] - df_size_weekly['Adjusted Sales Size']\n",
    "\n",
    "wmape = np.sum(np.abs(df_size_weekly['Difference'])) / np.sum(df_size_weekly['Demand']) * 100\n",
    "wmpe = np.sum(df_size_weekly['Difference']) / np.sum(df_size_weekly['Demand']) * 100\n",
    "\n",
    "wmape, wmpe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
