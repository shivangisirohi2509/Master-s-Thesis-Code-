{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('X Forecasting.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = data.groupby(['Calendar Day','Fiscal Year', 'Fiscal Week', 'Fiscal Year Week', 'Article: MH6 Sub Product Group']).agg({\n",
    "    'Adjusted Sales': 'sum',\n",
    "    'Month': lambda x: x.mode()[0]  # Use mode to select the most common month if there's any tie within the week\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Variables for Black Friday, Easter and Christmas Weeks\n",
    "\n",
    "# Mark Black Friday weeks\n",
    "black_friday_weeks = {2022: 43, 2023: 42}\n",
    "agg_data['Black_Friday'] = agg_data.apply(lambda row: 1 if row['Fiscal Week'] == black_friday_weeks[row['Fiscal Year']] else 0, axis=1)\n",
    "# Define the Easter weeks for both years\n",
    "easter_weeks = {\n",
    "    2022: [9, 10],\n",
    "    2023: [10, 11]\n",
    "}\n",
    "\n",
    "# Create a new column 'Easter_Week' in agg_data and set it to 0\n",
    "agg_data['Easter_Week'] = 0\n",
    "\n",
    "# Mark the Easter weeks as 1\n",
    "for year, weeks in easter_weeks.items():\n",
    "    for week in weeks:\n",
    "        agg_data.loc[(agg_data['Fiscal Year'] == year) & (agg_data['Fiscal Week'].isin(weeks)), 'Easter_Week'] = 1\n",
    "        \n",
    "#Mark Christmas Weeks as fiscal week 47 and 48\n",
    "christmas_weeks= {2022: [48,49], 2023: [48,49]}\n",
    "agg_data['Christmas_Week'] = 0\n",
    "for year, weeks in christmas_weeks.items():\n",
    "    for week in weeks:\n",
    "        agg_data.loc[(agg_data['Fiscal Year'] == year) & (agg_data['Fiscal Week'].isin(weeks)), 'Christmas_Week'] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant term for the intercept to the model\n",
    "X = sm.add_constant(agg_data[['Black_Friday', 'Easter_Week', 'Month', 'Christmas_Week']])  \n",
    "\n",
    "# Response variable\n",
    "y = agg_data['Adjusted Sales']\n",
    "\n",
    "# Fit OLS model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data.set_index('Calendar Day', inplace=True)\n",
    "agg_data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Iterate over each unique sub-product group\n",
    "for group in agg_data['Article: MH6 Sub Product Group'].unique():\n",
    "    # Filter data for the current subgroup\n",
    "    subgroup_data = agg_data[agg_data['Article: MH6 Sub Product Group'] == group]\n",
    "\n",
    "    # Define the sales data and fiscal year week for labeling\n",
    "    y = subgroup_data['Adjusted Sales']\n",
    "    fiscal_year_week = subgroup_data['Fiscal Year Week']\n",
    "\n",
    "    # Plot the time series data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y.index, y)\n",
    "    plt.title(f'Time Series Plot for {group}')\n",
    "    plt.xlabel('Fiscal Year Week')\n",
    "    plt.ylabel('Adjusted Sales')\n",
    "    plt.xticks(ticks=y.index[::2], labels=fiscal_year_week[::2], rotation=90)  # Adjust the tick frequency and rotation as needed\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ACF\n",
    "    plot_acf(y, lags=50)\n",
    "    plt.title(f'ACF Plot for {group}')\n",
    "    plt.xlabel('Lags')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot PACF\n",
    "    plot_pacf(y, lags=50)\n",
    "    plt.title(f'PACF Plot for {group}')\n",
    "    plt.xlabel('Lags')\n",
    "    plt.ylabel('Partial Autocorrelation')\n",
    "    plt.show()\n",
    "\n",
    "# Reset index after plotting\n",
    "agg_data.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from arch.unitroot import PhillipsPerron\n",
    "\n",
    "sub_product_groups = agg_data['Article: MH6 Sub Product Group'].unique()\n",
    "\n",
    "for group in sub_product_groups:\n",
    "    group_df = agg_data[agg_data['Article: MH6 Sub Product Group'] == group]\n",
    "    \n",
    "    # Assuming 'Adjusted Sales' is the column with the sales data\n",
    "    sales = group_df['Adjusted Sales']\n",
    "    \n",
    "    # Perform ADF test\n",
    "    adf_result = adfuller(sales.dropna())  # dropna() is used to remove any NaN values\n",
    "    print(f'ADF Statistic for group {group}: {adf_result[0]}')\n",
    "    print(f'p-value: {adf_result[1]}')\n",
    "    \n",
    "    # Perform KPSS test\n",
    "    kpss_result = kpss(sales.dropna(), regression='c', nlags='auto')\n",
    "    print(f'KPSS Statistic for group {group}: {kpss_result[0]}')\n",
    "    print(f'p-value: {kpss_result[1]}')\n",
    "    \n",
    "    # Perform PP test\n",
    "    pp = PhillipsPerron(sales.dropna())\n",
    "    print(f'PP Statistic for group {group}: {pp.stat}')\n",
    "    print(f'p-value: {pp.pvalue}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Create a dictionary to store results\n",
    "sarima_parameters = {}\n",
    "model_summaries = {}\n",
    "forecast_results = {}\n",
    "accuracy_metrics = {}\n",
    "forecast_df = pd.DataFrame()\n",
    "accuracy_df = pd.DataFrame(columns=['Subproduct Group', 'MAE', 'RMSE', 'MAPE', 'wMAPE'])\n",
    "\n",
    "# Assuming agg_data is predefined\n",
    "for group in agg_data['Article: MH6 Sub Product Group'].unique():\n",
    "    print(f\"Running auto_arima for subgroup: {group}\")\n",
    "\n",
    "    # Prepare the data\n",
    "    subgroup_data = agg_data[agg_data['Article: MH6 Sub Product Group'] == group]\n",
    "    subgroup_data = subgroup_data.sort_index()\n",
    "    y = subgroup_data['Adjusted Sales']\n",
    "    exog = subgroup_data[['Black_Friday', 'Easter_Week', 'Christmas_Week', 'Month']]\n",
    "    \n",
    "    try:\n",
    "        # Auto ARIMA to determine optimal parameters including seasonality\n",
    "        sarimax_model = auto_arima(y, exogenous=exog, start_p=0, start_q=0, max_p=4, max_q=4,\n",
    "                                   seasonal=True, m=52, trace=True, error_action='ignore', \n",
    "                                   suppress_warnings=True, stepwise=True, information_criterion='aic')\n",
    "    except ValueError as e:\n",
    "        if 'zero-size array to reduction operation maximum which has no identity' in str(e):\n",
    "            print(f\"Retrying for subgroup {group} with D=1 due to encountered error.\")\n",
    "            sarimax_model = auto_arima(y, exogenous=exog,\n",
    "                                       start_p=0, start_q=0, max_p=4, max_q=4,\n",
    "                                       start_P=0, start_Q=0, max_P=4, max_Q=4, m=52,\n",
    "                                       seasonal=True, trace=True, error_action='ignore',\n",
    "                                       D=1, d=None,  # Manually set seasonal differencing\n",
    "                                       suppress_warnings=True, stepwise=True,  \n",
    "                                       information_criterion='aic')\n",
    "        else:\n",
    "            raise e \n",
    "\n",
    "    # Store optimal parameters\n",
    "    sarima_parameters[group] = (sarimax_model.order, sarimax_model.seasonal_order)\n",
    "    print(f\"Optimal parameters for {group}: {sarima_parameters[group]}\")\n",
    "\n",
    "    split_point = int(len(subgroup_data) * 0.8)\n",
    "    train = subgroup_data.iloc[:split_point]\n",
    "    test = subgroup_data.iloc[split_point:]\n",
    "    exog_train = train[['Black_Friday', 'Easter_Week', 'Christmas_Week', 'Month']]\n",
    "    exog_test = test[['Black_Friday', 'Easter_Week', 'Christmas_Week', 'Month']]\n",
    "\n",
    "    # Fit SARIMAX model\n",
    "    order, seasonal_order = sarima_parameters[group]\n",
    "    model = SARIMAX(train['Adjusted Sales'], exog=exog_train, order=order, \n",
    "                    seasonal_order=seasonal_order, enforce_stationarity=False, \n",
    "                    enforce_invertibility=False)\n",
    "    results = model.fit(disp=False)\n",
    "    model_summaries[group] = results.summary()\n",
    "\n",
    "    # Forecasting\n",
    "    forecast = results.get_forecast(steps=len(test), exog=exog_test)\n",
    "    forecast_mean = forecast.predicted_mean\n",
    "    forecast_conf_int = forecast.conf_int()\n",
    "    forecast_results[group] = {'mean': forecast_mean, 'conf_int': forecast_conf_int}\n",
    "\n",
    "    # Calculate accuracy metrics\n",
    "    errors = forecast_mean - test['Adjusted Sales']\n",
    "    mae = mean_absolute_error(test['Adjusted Sales'], forecast_mean)\n",
    "    rmse = np.sqrt(mean_squared_error(test['Adjusted Sales'], forecast_mean))\n",
    "    mape = np.mean(np.abs(errors / test['Adjusted Sales'])) * 100\n",
    "    wmape = np.sum(np.abs(errors)) / np.sum(test['Adjusted Sales']) * 100\n",
    "    accuracy_metrics[group] = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'wMAPE': wmape}\n",
    "\n",
    "    # Append to DataFrames\n",
    "    accuracy_df = pd.concat([accuracy_df, pd.DataFrame({'Subproduct Group': [group], 'MAE': [mae], 'RMSE': [rmse], 'MAPE': [mape], 'wMAPE': [wmape]})])\n",
    "    group_forecast_df = pd.DataFrame({\n",
    "        'Fiscal Year Week': test.index,\n",
    "        'Actual': test['Adjusted Sales'].values,\n",
    "        'Forecast': forecast_mean.values,\n",
    "        'Subproduct Group': group\n",
    "    })\n",
    "    forecast_df = pd.concat([forecast_df, group_forecast_df])\n",
    "\n",
    "    # Plotting results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train.index, train['Adjusted Sales'], label='Training Data')\n",
    "    plt.plot(test.index, test['Adjusted Sales'], label='Actual Data')\n",
    "    plt.plot(test.index, forecast_mean, label='Forecast', color='red', linestyle='--')\n",
    "    plt.fill_between(test.index, forecast_conf_int.iloc[:, 0], forecast_conf_int.iloc[:, 1], color='pink', alpha=0.3)\n",
    "    plt.title(f'SARIMAX Forecast vs Actuals for {group}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Accuracy metrics for {group}: {accuracy_metrics[group]}\")\n",
    "    print(f\"Model summary for {group}:\\n{model_summaries[group]}\")\n",
    "\n",
    "print(\"Modeling and evaluation completed for all subproduct groups.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Additional plotting for all subproduct groups using forecast_df\n",
    "for group in forecast_df['Subproduct Group'].unique():\n",
    "    group_data = forecast_df[forecast_df['Subproduct Group'] == group]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(group_data['Fiscal Year Week'], group_data['Actual'], label='Actual Data')\n",
    "    plt.plot(group_data['Fiscal Year Week'], group_data['Forecast'], label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'Actual vs Forecast for {group}')\n",
    "    plt.xlabel('Fiscal Year Week')\n",
    "    plt.ylabel('Sales')\n",
    "    # Set x-ticks to the exact 'Fiscal Year Week' values with a step size of 1 and rotation of 90 degrees\n",
    "    start = group_data['Fiscal Year Week'].min()\n",
    "    end = group_data['Fiscal Year Week'].max()\n",
    "    plt.xticks(np.arange(start, end, 1), rotation=90)\n",
    "    \n",
    "    # Format the x-tick labels with a precision of 4 decimal places\n",
    "    formatter = ticker.FuncFormatter(lambda x, pos: '{:.4f}'.format(x))\n",
    "    plt.gca().xaxis.set_major_formatter(formatter)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holt's Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Define the sub product groups\n",
    "sub_product_groups = agg_data['Article: MH6 Sub Product Group'].unique()\n",
    "\n",
    "# Create a plot for each sub product group\n",
    "for sub_product_group in sub_product_groups:\n",
    "    # Filter the data for the sub product group\n",
    "    sub_data = agg_data[agg_data['Article: MH6 Sub Product Group'] == sub_product_group]['Adjusted Sales']\n",
    "    \n",
    "    # Perform seasonal decomposition\n",
    "    result = seasonal_decompose(sub_data, period=52)  # Assuming 53 weeks in a year\n",
    "    \n",
    "    # Plot the decomposition\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(8, 6), sharex=True)\n",
    "    result.observed.plot(ax=axes[0], title='Observed')\n",
    "    result.trend.plot(ax=axes[1], title='Trend')\n",
    "    result.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "    result.resid.plot(ax=axes[3], title='Residual')\n",
    "    \n",
    "    # Adjust layout and add a super title\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    fig.suptitle(f'Seasonal Decomposition of {sub_product_group}', fontsize=12)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the parameter grid\n",
    "trend_options = [None, 'add', 'mul']\n",
    "seasonal_options = ['add', 'mul']\n",
    "seasonal_periods_options = [52]  \n",
    "\n",
    "# Function to evaluate a model and return its AIC\n",
    "def evaluate_model(data, trend, seasonal, seasonal_periods):\n",
    "    model = ExponentialSmoothing(data, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods)\n",
    "    hw_fit = model.fit(optimized=True)\n",
    "    return hw_fit.aic, hw_fit\n",
    "\n",
    "# Dictionary to store the best model parameters for each subproduct group\n",
    "best_model_params = {}\n",
    "\n",
    "# DataFrame to store the residuals for each subproduct group\n",
    "residuals_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each unique subproduct group\n",
    "for group in agg_data['Article: MH6 Sub Product Group'].unique():\n",
    "    print(f\"Running grid search for Holt-Winters model for subgroup: {group}\")\n",
    "\n",
    "    # Filter data for the current subgroup\n",
    "    subgroup_data = agg_data[agg_data['Article: MH6 Sub Product Group'] == group]\n",
    "    subgroup_data.sort_index(inplace=True)\n",
    "\n",
    "    # Extract the Adjusted Sales data\n",
    "    data = subgroup_data['Adjusted Sales']\n",
    "\n",
    "    # Initialize variables to store the best model and AIC\n",
    "    best_aic = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    # Perform grid search over the parameter combinations\n",
    "    for trend, seasonal, seasonal_periods in product(trend_options, seasonal_options, seasonal_periods_options):\n",
    "        try:\n",
    "            aic, model = evaluate_model(data, trend, seasonal, seasonal_periods)\n",
    "            if aic < best_aic:\n",
    "                best_aic = aic\n",
    "                best_params = {'trend': trend, 'seasonal': seasonal, 'seasonal_periods': seasonal_periods}\n",
    "                best_model = model\n",
    "        except Exception as e:\n",
    "            print(f\"Model fitting failed for parameters trend={trend}, seasonal={seasonal}, seasonal_periods={seasonal_periods}: {e}\")\n",
    "            continue\n",
    "\n",
    "    best_model_params[group] = best_params\n",
    "    print(f\"Best parameters for {group}: trend={best_params['trend']}, seasonal={best_params['seasonal']}, seasonal_periods={best_params['seasonal_periods']}\")\n",
    "\n",
    "    # Add the residuals to the residuals DataFrame\n",
    "    residuals_df[group] = best_model.resid\n",
    "\n",
    "    # Plot the fitted values vs actual values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data.index, data, label='Actual Data')\n",
    "    plt.plot(data.index, best_model.fittedvalues, label='Fitted Values', color='red', linestyle='--')\n",
    "    plt.title(f'Actual vs Fitted Values for {group}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data.index, best_model.resid, label='Residuals', color='blue')\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.title(f'Residuals of the Model for {group}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ACF of residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_acf(best_model.resid, lags=40)\n",
    "    plt.title(f'ACF of Residuals for {group}')\n",
    "    plt.xlabel('Lags')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.show()\n",
    "\n",
    "# Print best parameters for each group\n",
    "print(\"Best parameters for each subproduct group:\")\n",
    "print(best_model_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dictionaries to store the results\n",
    "hw_model_results = {}\n",
    "hw_model_summaries = {}\n",
    "hw_forecast_results = {}\n",
    "hw_accuracy_metrics = {}\n",
    "\n",
    "# DataFrames to store forecast and accuracy metrics\n",
    "hw_forecast_df = pd.DataFrame()\n",
    "hw_accuracy_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each unique subproduct group\n",
    "for group in agg_data['Article: MH6 Sub Product Group'].unique():\n",
    "    print(f\"Running Holt's Winter Exponential Smoothing for subgroup: {group}\")\n",
    "\n",
    "    # Filter data for the current subgroup\n",
    "    subgroup_data = agg_data[agg_data['Article: MH6 Sub Product Group'] == group]\n",
    "\n",
    "    # Ensure the DataFrame is sorted by index\n",
    "    subgroup_data.sort_index(inplace=True)\n",
    "\n",
    "    # Calculate the split point\n",
    "    split_point = int(len(subgroup_data) * 0.8)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train = subgroup_data.iloc[:split_point]\n",
    "    test = subgroup_data.iloc[split_point:] \n",
    "\n",
    "    # Get the best parameters for the current subgroup\n",
    "    best_params = best_model_params[group]\n",
    "    \n",
    "    # Fit the model using the best parameters\n",
    "    model = ExponentialSmoothing(train['Adjusted Sales'],\n",
    "                                 trend=best_params['trend'],\n",
    "                                 seasonal=best_params['seasonal'],\n",
    "                                 seasonal_periods=best_params['seasonal_periods'])\n",
    "    results = model.fit()\n",
    "\n",
    "    # Store the model and summary\n",
    "    hw_model_results[group] = results\n",
    "    hw_model_summaries[group] = results.summary()\n",
    "\n",
    "    # Forecasting\n",
    "    forecast = results.forecast(steps=len(test))\n",
    "\n",
    "    # Store the forecast\n",
    "    hw_forecast_results[group] = forecast\n",
    "\n",
    "    # Calculate the errors\n",
    "    errors = forecast - test['Adjusted Sales']\n",
    "\n",
    "    # Calculate the accuracy metrics\n",
    "    mae = mean_absolute_error(test['Adjusted Sales'], forecast)\n",
    "    rmse = np.sqrt(mean_squared_error(test['Adjusted Sales'], forecast))\n",
    "    mape = np.mean(np.abs(errors / test['Adjusted Sales'])) * 100\n",
    "    wmape = np.sum(np.abs(errors)) / np.sum(test['Adjusted Sales']) * 100\n",
    "\n",
    "    # Store the accuracy metrics\n",
    "    hw_accuracy_metrics[group] = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'wMAPE': wmape}\n",
    "\n",
    "    # Append the accuracy metrics to the Holt-Winters accuracy DataFrame\n",
    "    hw_accuracy_df = pd.concat([hw_accuracy_df, pd.DataFrame({'Subproduct Group': [group], 'HW_MAE': [mae], 'HW_RMSE': [rmse], 'HW_MAPE': [mape], 'HW_wMAPE': [wmape]})])\n",
    "\n",
    "    # Create a DataFrame to store forecast and actual values for this subgroup\n",
    "    hw_group_forecast_df = pd.DataFrame({\n",
    "        'Fiscal Year Week': test['Fiscal Year Week'],\n",
    "        'Actual': test['Adjusted Sales'],\n",
    "        'Forecast': forecast,\n",
    "        'Subproduct Group': group\n",
    "    })\n",
    "\n",
    "    # Append the forecast data to the Holt-Winters forecast DataFrame\n",
    "    hw_forecast_df = pd.concat([hw_forecast_df, hw_group_forecast_df])\n",
    "    \n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    train['Adjusted Sales'].plot(label='Training Data')\n",
    "    test['Adjusted Sales'].plot(label='Actual Data')\n",
    "    forecast.plot(label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f\"Holt's Winter Exponential Smoothing Forecast vs Actuals for {group}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the summary and accuracy metrics\n",
    "    print(f\"Model summary for {group}:\\n{model_summaries[group]}\")\n",
    "    print(f\"Accuracy metrics for {group}:\\n{accuracy_metrics[group]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Additional plotting for all subproduct groups using forecast_df\n",
    "for group in hw_forecast_df['Subproduct Group'].unique():\n",
    "    group_data = hw_forecast_df[hw_forecast_df['Subproduct Group'] == group]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(group_data['Fiscal Year Week'], group_data['Actual'], label='Actual Data')\n",
    "    plt.plot(group_data['Fiscal Year Week'], group_data['Forecast'], label='Forecast', color='red', linestyle='--')\n",
    "    plt.title(f'Actual vs Forecast for {group}')\n",
    "    plt.xlabel('Fiscal Year Week')\n",
    "    plt.ylabel('Sales')\n",
    "    # Set x-ticks to the exact 'Fiscal Year Week' values with a step size of 1 and rotation of 90 degrees\n",
    "    start = group_data['Fiscal Year Week'].min()\n",
    "    end = group_data['Fiscal Year Week'].max()\n",
    "    plt.xticks(np.arange(start, end, 1), rotation=90)\n",
    "    \n",
    "    # Format the x-tick labels with a precision of 4 decimal places\n",
    "    formatter = ticker.FuncFormatter(lambda x, pos: '{:.4f}'.format(x))\n",
    "    plt.gca().xaxis.set_major_formatter(formatter)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function definitions\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def weighted_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(y_true) * 100\n",
    "\n",
    "def bias(y_true, y_pred):\n",
    "    return np.mean(y_true - y_pred)\n",
    "\n",
    "# WMAPE scorer\n",
    "wmape_scorer = make_scorer(weighted_mean_absolute_percentage_error, greater_is_better=False)\n",
    "\n",
    "# Data preparation\n",
    "data = agg_data.copy()  # Ensure you have loaded your data into 'agg_data'\n",
    "models = {}\n",
    "rf_forecast_df = pd.DataFrame()\n",
    "rf_accuracy_df = pd.DataFrame()\n",
    "feature_importance_rf = pd.DataFrame()\n",
    "\n",
    "for group in data['Article: MH6 Sub Product Group'].unique():\n",
    "    print(f\"Training model for subgroup: {group}\")\n",
    "    subgroup_data = data[data['Article: MH6 Sub Product Group'] == group]\n",
    "    \n",
    "    #Features for Cyclicity\n",
    "    subgroup_data['sin_fiscal_week'] = np.sin(2 * np.pi * subgroup_data['Fiscal Week'] / 52)\n",
    "    subgroup_data['cos_fiscal_week'] = np.cos(2 * np.pi * subgroup_data['Fiscal Week'] / 52)\n",
    "    \n",
    "    \n",
    "    X = subgroup_data[['Fiscal Year', 'Fiscal Week', 'Month', 'Black_Friday', 'Easter_Week', 'Christmas_Week', 'sin_fiscal_week', 'cos_fiscal_week']]\n",
    "    y = subgroup_data['Adjusted Sales']\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Initialize features for X_test\n",
    "    for lag in range(1, 5):\n",
    "        X_test[f'lag_{lag}_week'] = np.concatenate((y_train.tail(lag).values, y_test.head(len(y_test) - lag)))\n",
    "    for window in [2, 4]:\n",
    "        rolling_series = pd.concat([y_train.tail(window-1), y_test])\n",
    "        X_test[f'Rolling_Mean_{window}'] = rolling_series.rolling(window).mean().iloc[window-1:]\n",
    "        X_test[f'Rolling_Std_{window}'] = rolling_series.rolling(window).std().iloc[window-1:]\n",
    "\n",
    "    # Cross-validation setup\n",
    "    min_train_weeks = 52\n",
    "    val_weeks = 8\n",
    "    n_folds = (len(X_train) - min_train_weeks) // val_weeks\n",
    "    tscv = TimeSeriesSplit(n_splits=n_folds, test_size=val_weeks)\n",
    "    param_grid = {\n",
    "        'max_depth': [None, 3, 5, 7, 10],\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=wmape_scorer, cv=tscv, verbose=1)\n",
    "\n",
    "    # Cross-validation\n",
    "    for i, (train_index, val_index) in enumerate(tscv.split(X_train)):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        # Add features for each fold\n",
    "        for lag in range(1, 5):\n",
    "            X_train_fold[f'lag_{lag}_week'] = y_train_fold.shift(lag)\n",
    "            X_val_fold[f'lag_{lag}_week'] = np.concatenate((y_train_fold.tail(lag).values, y_val_fold.head(len(y_val_fold) - lag)))\n",
    "        for window in [2, 4]:\n",
    "            X_train_fold[f'Rolling_Mean_{window}'] = y_train_fold.rolling(window=window).mean()\n",
    "            X_train_fold[f'Rolling_Std_{window}'] = y_train_fold.rolling(window=window).std()\n",
    "            temp_series = pd.concat([y_train_fold.tail(window-1), y_val_fold])\n",
    "            X_val_fold[f'Rolling_Mean_{window}'] = temp_series.rolling(window=window).mean().iloc[window-1:]\n",
    "            X_val_fold[f'Rolling_Std_{window}'] = temp_series.rolling(window=window).std().iloc[window-1:]\n",
    "            \n",
    "        # Impute strategy: mean, median, most_frequent, or constant (fill with a specific value)\n",
    "        imputer = SimpleImputer(strategy='mean')  # or 'median', 'most_frequent'\n",
    "\n",
    "        # Impute missing values\n",
    "        X_train_fold_imputed = pd.DataFrame(imputer.fit_transform(X_train_fold), columns=X_train_fold.columns)\n",
    "        X_val_fold_imputed = pd.DataFrame(imputer.transform(X_val_fold), columns=X_val_fold.columns)\n",
    "\n",
    "        grid_search.fit(X_train_fold_imputed, y_train_fold)\n",
    "        y_pred_val = grid_search.predict(X_val_fold_imputed)\n",
    "        wmape_val = weighted_mean_absolute_percentage_error(y_val_fold, y_pred_val)\n",
    "        print(f\"Fold {i+1} - Validation wMAPE: {wmape_val}\")\n",
    "\n",
    "    # Best model and predictions\n",
    "    best_model = grid_search.best_estimator_\n",
    "    models[group] = best_model\n",
    "    print(f\"Best hyperparameters for {group}: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Print feature importances\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    print(f\"Feature importances for {group}:\")\n",
    "    for feature, importance in zip(X_train_fold_imputed.columns, feature_importances):\n",
    "        print(f\"{feature}: {importance}\")\n",
    "    \n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    wmape = weighted_mean_absolute_percentage_error(y_test, y_pred)\n",
    "    bias_val = bias(y_test, y_pred)\n",
    "    print(f\"Test Set Evaluation for {group}:\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(f\"Mean Absolute Percentage Error: {mape}\")\n",
    "    print(f\"Weighted Mean Absolute Percentage Error: {wmape}\")\n",
    "    print(f\"Bias: {bias_val}\")\n",
    "\n",
    "    # Append metrics and forecast\n",
    "    rf_accuracy_df = pd.concat([rf_accuracy_df, pd.DataFrame({'Subproduct Group': [group], 'RF_MAPE': [mape], 'RF_RMSE': [rmse], 'RF_wMAPE': [wmape]})])\n",
    "    rf_group_forecast_df = pd.DataFrame({\n",
    "        'Fiscal Year Week': X_test.index,\n",
    "        'Actual': y_test.values,\n",
    "        'Forecast': y_pred,\n",
    "        'Subproduct Group': group\n",
    "    })\n",
    "    rf_forecast_df = pd.concat([rf_forecast_df, rf_group_forecast_df])\n",
    "\n",
    "    #Save feature importances\n",
    "    imp_rf = pd.DataFrame({\n",
    "                'Feature': X_train_fold_imputed.columns,\n",
    "                'Importance': best_model.feature_importances_,\n",
    "                'Subproduct Group': group\n",
    "            })\n",
    "    feature_importance_rf = pd.concat([feature_importance_rf, imp_rf], ignore_index=True)\n",
    "    \n",
    "    X_test['Fiscal Year Week'] = X_test['Fiscal Year'].astype(str) + ' - ' + X_test['Fiscal Week'].astype(str)\n",
    "\n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(X_test['Fiscal Year Week'], y_test.values, label='Actual')\n",
    "    plt.plot(X_test['Fiscal Year Week'], y_pred, label='Predicted')\n",
    "    plt.title(f\"Actual vs Predicted Sales for {group}\")\n",
    "    plt.xlabel('Fiscal Year Week')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Aggregate and visualize feature importances according to ascending order for each group\n",
    "for grp in feature_importance_rf['Subproduct Group'].unique():\n",
    "    group_data = feature_importance_rf[feature_importance_rf['Subproduct Group'] == grp].sort_values(by='Importance', ascending=False)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=group_data, x='Importance', y='Feature', ci=None)\n",
    "    plt.title(f'Feature Importances for {grp}')\n",
    "    plt.show()\n",
    "\n",
    "# Overall feature importances\n",
    "overall_importances = feature_importance_rf.groupby('Feature')['Importance'].mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=overall_importances.values, y=overall_importances.index, orient='h')\n",
    "plt.title('Average Feature Importances Across All Groups')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define scoring functions\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def weighted_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(y_true) * 100\n",
    "\n",
    "def bias(y_true, y_pred):\n",
    "    return np.mean(y_true - y_pred)\n",
    "\n",
    "# WMAPE scorer\n",
    "wmape_scorer = make_scorer(weighted_mean_absolute_percentage_error, greater_is_better=False)\n",
    "\n",
    "# Data preparation\n",
    "data = agg_data.copy()  # Assume 'agg_data' is previously loaded\n",
    "models = {}\n",
    "feature_importance_df = pd.DataFrame()\n",
    "xgb_forecast_df = pd.DataFrame()\n",
    "xgb_accuracy_df = pd.DataFrame()\n",
    "\n",
    "for group in data['Article: MH6 Sub Product Group'].unique():\n",
    "    print(f\"Training model for subgroup: {group}\")\n",
    "    subgroup_data = data[data['Article: MH6 Sub Product Group'] == group]\n",
    "    \n",
    "    subgroup_data['sin_fiscal_week'] = np.sin(2 * np.pi * subgroup_data['Fiscal Week'] / 52)\n",
    "    subgroup_data['cos_fiscal_week'] = np.cos(2 * np.pi * subgroup_data['Fiscal Week'] / 52)\n",
    "    \n",
    "    X = subgroup_data[['Fiscal Year', 'Fiscal Week', 'Month', 'Black_Friday', 'Easter_Week', 'Christmas_Week', 'sin_fiscal_week', 'cos_fiscal_week']]\n",
    "    y = subgroup_data['Adjusted Sales']\n",
    "    \n",
    "    #Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Initialize features for X_test\n",
    "    feature_names = ['lag_1_week', 'lag_2_week', 'lag_3_week', 'lag_4_week', 'Rolling_Mean_2', 'Rolling_Mean_4', 'Rolling_Std_2', 'Rolling_Std_4']\n",
    "    for lag in range(1, 5):\n",
    "        X_test[f'lag_{lag}_week'] = np.concatenate((y_train.tail(lag).values, y_test.head(len(y_test) - lag)))\n",
    "    \n",
    "    for window in [2, 4]:\n",
    "        rolling_series = pd.concat([y_train.tail(window-1), y_test])\n",
    "        X_test[f'Rolling_Mean_{window}'] = rolling_series.rolling(window).mean().iloc[window-1:]\n",
    "        X_test[f'Rolling_Std_{window}'] = rolling_series.rolling(window).std().iloc[window-1:]\n",
    "\n",
    "    # Cross-validation setup\n",
    "    min_train_weeks = 52\n",
    "    val_weeks = 8\n",
    "    n_folds = (len(X_train) - min_train_weeks) // val_weeks\n",
    "    tscv = TimeSeriesSplit(n_splits=n_folds, test_size=val_weeks)\n",
    "    param_grid = {\n",
    "        'max_depth': [None, 3, 5, 7, 10],\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    model = XGBRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=wmape_scorer, cv=tscv, verbose=1)\n",
    "\n",
    "    # Cross-validation\n",
    "    for i, (train_index, val_index) in enumerate(tscv.split(X_train)):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # Add features for each fold\n",
    "        for lag in range(1, 5):\n",
    "            X_train_fold[f'lag_{lag}_week'] = y_train_fold.shift(lag)\n",
    "            X_val_fold[f'lag_{lag}_week'] = np.concatenate((y_train_fold.tail(lag).values, y_val_fold.head(len(y_val_fold) - lag)))\n",
    "        \n",
    "        for window in [2, 4]:\n",
    "            X_train_fold[f'Rolling_Mean_{window}'] = y_train_fold.rolling(window=window).mean()\n",
    "            X_train_fold[f'Rolling_Std_{window}'] = y_train_fold.rolling(window=window).std()\n",
    "            temp_series = pd.concat([y_train_fold.tail(window-1), y_val_fold])\n",
    "            X_val_fold[f'Rolling_Mean_{window}'] = temp_series.rolling(window=window).mean().iloc[window-1:]\n",
    "            X_val_fold[f'Rolling_Std_{window}'] = temp_series.rolling(window=window).std().iloc[window-1:]\n",
    "\n",
    "        grid_search.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_val = grid_search.predict(X_val_fold)\n",
    "        wmape_val = weighted_mean_absolute_percentage_error(y_val_fold, y_pred_val)\n",
    "        print(f\"Fold {i+1} - Validation wMAPE: {wmape_val}\")\n",
    "        \n",
    "        \n",
    "    # Best model and predictions\n",
    "    best_model = grid_search.best_estimator_\n",
    "    models[group] = best_model\n",
    "    print(f\"Best hyperparameters for {group}: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Print feature importances\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    print(f\"Feature importances for {group}:\")\n",
    "    for feature, importance in zip(X_train_fold.columns, feature_importances):\n",
    "        print(f\"{feature}: {importance}\")\n",
    "    \n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    wmape = weighted_mean_absolute_percentage_error(y_test, y_pred)\n",
    "    bias_val = bias(y_test, y_pred)\n",
    "    print(f\"Test Set Evaluation for {group}:\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(f\"Mean Absolute Percentage Error: {mape}\")\n",
    "    print(f\"Weighted Mean Absolute Percentage Error: {wmape}\")\n",
    "    print(f\"Bias: {bias_val}\")\n",
    "\n",
    "    # Append metrics and forecast\n",
    "    xgb_accuracy_df = pd.concat([xgb_accuracy_df, pd.DataFrame({'Subproduct Group': [group], 'XGB_MAPE': [mape], 'XGB_RMSE': [rmse], 'XGB_wMAPE': [wmape]})])\n",
    "    xgb_group_forecast_df = pd.DataFrame({\n",
    "            'Fiscal Year Week': X_test.index,\n",
    "            'Actual': y_test.values,\n",
    "            'Forecast': y_pred,\n",
    "            'Subproduct Group': group\n",
    "        })\n",
    "    xgb_forecast_df = pd.concat([xgb_forecast_df, xgb_group_forecast_df])\n",
    "        \n",
    "    # Save feature importances\n",
    "    imp_df = pd.DataFrame({\n",
    "                'Feature': X_train_fold.columns,\n",
    "                'Importance': best_model.feature_importances_,\n",
    "                'Subproduct Group': group\n",
    "            })\n",
    "    feature_importance_df = pd.concat([feature_importance_df, imp_df], ignore_index=True)\n",
    "\n",
    " \n",
    "    X_test['Fiscal Year Week'] = X_test['Fiscal Year'].astype(str) + ' - ' + X_test['Fiscal Week'].astype(str)\n",
    "        \n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(X_test['Fiscal Year Week'], y_test.values, label='Actual')\n",
    "    plt.plot(X_test['Fiscal Year Week'], y_pred, label='Predicted')\n",
    "    plt.title(f\"Actual vs Predicted Sales for {group}\")\n",
    "    plt.xlabel('Fiscal Year Week')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Aggregate and visualize feature importances according to ascending order for each group\n",
    "for grp in feature_importance_df['Subproduct Group'].unique():\n",
    "    group_data = feature_importance_df[feature_importance_df['Subproduct Group'] == grp].sort_values(by='Importance', ascending=False)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=group_data, x='Importance', y='Feature', ci=None)\n",
    "    plt.title(f'Feature Importances for {grp}')\n",
    "    plt.show()\n",
    "\n",
    "# Overall feature importances\n",
    "overall_importances = feature_importance_df.groupby('Feature')['Importance'].mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=overall_importances.values, y=overall_importances.index, orient='h')\n",
    "plt.title('Average Feature Importances Across All Groups')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
